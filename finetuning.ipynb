{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "execution_state": "idle",
   "id": "1de26902-96d9-4f3b-8458-054fbe68b00b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T07:03:49.947770Z",
     "iopub.status.busy": "2025-09-05T07:03:49.947570Z",
     "iopub.status.idle": "2025-09-05T07:03:54.969285Z",
     "shell.execute_reply": "2025-09-05T07:03:54.968746Z",
     "shell.execute_reply.started": "2025-09-05T07:03:49.947755Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "def cleanup_globals(vars_to_keep: list):\n",
    "    global_vars = list(globals().keys())\n",
    "    protected_vars = ['In', 'Out', 'get_ipython', 'exit', 'quit', 'gc', 'torch', 'cleanup_globals']\n",
    "\n",
    "    for var in global_vars:\n",
    "        if var not in vars_to_keep and not var.startswith('_') and var not in protected_vars:\n",
    "            try:\n",
    "                del globals()[var]\n",
    "                print(f\"{var} ì‚­ì œë¨\")\n",
    "            except:\n",
    "                continue\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "execution_state": "idle",
   "id": "68e91c4e-1ab2-4aa1-a583-b759227a7dc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T03:23:01.178461Z",
     "iopub.status.busy": "2025-09-05T03:23:01.178236Z",
     "iopub.status.idle": "2025-09-05T03:23:03.105600Z",
     "shell.execute_reply": "2025-09-05T03:23:03.104955Z",
     "shell.execute_reply.started": "2025-09-05T03:23:01.178444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ë©”ëª¨ë¦¬ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.\n",
      "íŒŒì¼ì„ pandas dataframe ë¡œ ë³€í™˜\n",
      "ì›ë³¸ ë°ì´í„° ìƒìœ„ 5ê°œ\n",
      "<bound method NDFrame.head of                                                template  \\\n",
      "0     {\"title\": \"íšŒì‚¬ì†Œê°œì„œ ë°œì†¡\", \"text\": \"ì•ˆë…•í•˜ì„¸ìš” #{ìˆ˜ì‹ ìëª…}ë‹˜,...   \n",
      "1     {\"title\": \"ì„œë¹„ìŠ¤ ì†Œê°œì„œ ë°œì†¡\", \"text\": \"ì•ˆë…•í•˜ì„¸ìš” #{ìˆ˜ì‹ ìëª…}...   \n",
      "2     {\"title\": \"(ì „ìš©) ê°•ì˜ ì¼ì • ì•ˆë‚´ / í™”ì¼€í„°\", \"text\": \"ì•ˆë…•í•˜ì„¸...   \n",
      "3     {\"title\": \"(ê³µìš©) í›„ê¸° ì‘ì„± ìš”ì²­_ì´ë¯¸ì§€í˜•_01\", \"text\": \"[í…œ...   \n",
      "4     {\"title\": \"(ê³µìš©) ì¸ë³´ì´ìŠ¤ ì•Œë¦¼_ì´ë¯¸ì§€í˜•_01\", \"text\": \"â–  #...   \n",
      "...                                                 ...   \n",
      "1167  {\"title\": \"ì¬ì…ê³  ì•Œë¦¼\", \"text\": \"ì•ˆë…•í•˜ì„¸ìš” #{ìˆ˜ì‹ ìëª…}ë‹˜,\\n...   \n",
      "1168  {\"title\": \"ì‚¬ì „ êµ¬ë§¤ ì˜ˆì•½\", \"text\": \"ì•ˆë…•í•˜ì„¸ìš” #{ìˆ˜ì‹ ìëª…}ë‹˜,...   \n",
      "1169  {\"title\": \"ì •ê¸°êµ¬ë… ê²°ì œ ì•ˆë‚´\", \"text\": \"ì•ˆë…•í•˜ì„¸ìš” #{ìˆ˜ì‹ ìëª…}...   \n",
      "1170  {\"title\": \"ìë™ ê²°ì œ ì •ë³´ ë“±ë¡ ì™„ë£Œ\", \"text\": \"ì•ˆë…•í•˜ì„¸ìš” #{ìˆ˜...   \n",
      "1171  {\"title\": \"ìë™ ê²°ì œ ì•ˆë‚´\", \"text\": \"ì•ˆë…•í•˜ì„¸ìš” #{ìˆ˜ì‹ ìëª…}ë‹˜,...   \n",
      "\n",
      "                                user_input  \\\n",
      "0             ì•ˆë…•í•˜ì„¸ìš”, íšŒì‚¬ì†Œê°œì„œ ë°œì†¡ í…œí”Œë¦¿ ì œì‘ ë¶€íƒë“œë ¤ìš”   \n",
      "1                           ì„œë¹„ìŠ¤ ì†Œê°œì„œ ë°œì†¡ í…œí”Œë¦¿   \n",
      "2        (ì „ìš©) ê°•ì˜ ì¼ì • ì•ˆë‚´ / í™”ì¼€í„°ìš© ì•Œë¦¼í†¡ì„ ë§Œë“¤ê³  ì‹¶ì–´ìš”   \n",
      "3             (ê³µìš©) í›„ê¸° ì‘ì„± ìš”ì²­_ì´ë¯¸ì§€í˜•_01 ì•ˆë‚´ ë©”ì‹œì§€   \n",
      "4       (ê³µìš©) ì¸ë³´ì´ìŠ¤ ì•Œë¦¼_ì´ë¯¸ì§€í˜•_01ìš© ì•Œë¦¼í†¡ì„ ë§Œë“¤ê³  ì‹¶ì–´ìš”   \n",
      "...                                    ...   \n",
      "1167      ê³ ê°ì—ê²Œ ë³´ë‚¼ ì¬ì…ê³  ì•Œë¦¼ ë©”ì‹œì§€ë¥¼ ì‘ì„±í•´ì£¼ì‹¤ ìˆ˜ ìˆë‚˜ìš”?   \n",
      "1168                       ì‚¬ì „ êµ¬ë§¤ ì˜ˆì•½ ì•Œë¦¼í†¡ ì œì‘   \n",
      "1169  ì •ê¸°êµ¬ë… ê²°ì œ ì•ˆë‚´ ì•Œë¦¼ ë©”ì‹œì§€ê°€ í•„ìš”í•œë° ë„ì›€ ì£¼ì‹¤ ìˆ˜ ìˆë‚˜ìš”?   \n",
      "1170    ì•ˆë…•í•˜ì„¸ìš”! ìë™ ê²°ì œ ì •ë³´ ë“±ë¡ ì™„ë£Œ ê´€ë ¨ í…œí”Œë¦¿ì´ í•„ìš”í•´ìš”   \n",
      "1171                       ìë™ ê²°ì œ ì•ˆë‚´ í…œí”Œë¦¿ ìƒì„±   \n",
      "\n",
      "                                                 policy  \n",
      "0     ì •ë³´ì„± ë©”ì‹œì§€ë€ ì •ë³´í†µì‹ ë§ë²• ì•ˆë‚´ì„œì— 'ì˜ë¦¬ëª©ì  ê´‘ê³ ì„± ì •ë³´ì˜ ì˜ˆì™¸'ì— í•´ë‹¹í•˜ëŠ” ë©”...  \n",
      "1     ì •ë³´ì„± ë©”ì‹œì§€ë€ ì •ë³´í†µì‹ ë§ë²• ì•ˆë‚´ì„œì— 'ì˜ë¦¬ëª©ì  ê´‘ê³ ì„± ì •ë³´ì˜ ì˜ˆì™¸'ì— í•´ë‹¹í•˜ëŠ” ë©”...  \n",
      "2     ì •ë³´ì„± ë©”ì‹œì§€ë€ ì •ë³´í†µì‹ ë§ë²• ì•ˆë‚´ì„œì— 'ì˜ë¦¬ëª©ì  ê´‘ê³ ì„± ì •ë³´ì˜ ì˜ˆì™¸'ì— í•´ë‹¹í•˜ëŠ” ë©”...  \n",
      "3     ì •ë³´ì„± ë©”ì‹œì§€ë€ ì •ë³´í†µì‹ ë§ë²• ì•ˆë‚´ì„œì— 'ì˜ë¦¬ëª©ì  ê´‘ê³ ì„± ì •ë³´ì˜ ì˜ˆì™¸'ì— í•´ë‹¹í•˜ëŠ” ë©”...  \n",
      "4     ì •ë³´ì„± ë©”ì‹œì§€ë€ ì •ë³´í†µì‹ ë§ë²• ì•ˆë‚´ì„œì— 'ì˜ë¦¬ëª©ì  ê´‘ê³ ì„± ì •ë³´ì˜ ì˜ˆì™¸'ì— í•´ë‹¹í•˜ëŠ” ë©”...  \n",
      "...                                                 ...  \n",
      "1167  ì •ë³´ì„± ë©”ì‹œì§€ë€ ì •ë³´í†µì‹ ë§ë²• ì•ˆë‚´ì„œì— 'ì˜ë¦¬ëª©ì  ê´‘ê³ ì„± ì •ë³´ì˜ ì˜ˆì™¸'ì— í•´ë‹¹í•˜ëŠ” ë©”...  \n",
      "1168  ì •ë³´ì„± ë©”ì‹œì§€ë€ ì •ë³´í†µì‹ ë§ë²• ì•ˆë‚´ì„œì— 'ì˜ë¦¬ëª©ì  ê´‘ê³ ì„± ì •ë³´ì˜ ì˜ˆì™¸'ì— í•´ë‹¹í•˜ëŠ” ë©”...  \n",
      "1169  ì •ë³´ì„± ë©”ì‹œì§€ë€ ì •ë³´í†µì‹ ë§ë²• ì•ˆë‚´ì„œì— 'ì˜ë¦¬ëª©ì  ê´‘ê³ ì„± ì •ë³´ì˜ ì˜ˆì™¸'ì— í•´ë‹¹í•˜ëŠ” ë©”...  \n",
      "1170  ì •ë³´ì„± ë©”ì‹œì§€ë€ ì •ë³´í†µì‹ ë§ë²• ì•ˆë‚´ì„œì— 'ì˜ë¦¬ëª©ì  ê´‘ê³ ì„± ì •ë³´ì˜ ì˜ˆì™¸'ì— í•´ë‹¹í•˜ëŠ” ë©”...  \n",
      "1171  ì •ë³´ì„± ë©”ì‹œì§€ë€ ì •ë³´í†µì‹ ë§ë²• ì•ˆë‚´ì„œì— 'ì˜ë¦¬ëª©ì  ê´‘ê³ ì„± ì •ë³´ì˜ ì˜ˆì™¸'ì— í•´ë‹¹í•˜ëŠ” ë©”...  \n",
      "\n",
      "[1172 rows x 3 columns]>\n",
      "\n",
      "Hugging Face Dataset ìœ¼ë¡œ ë³€í™˜\n",
      "ìµœì¢… ë¶„í• ëœ ë°ì´í„° ì…‹\n",
      "Dataset({\n",
      "    features: ['template', 'user_input', 'policy'],\n",
      "    num_rows: 937\n",
      "})\n",
      "Dataset({\n",
      "    features: ['template', 'user_input', 'policy'],\n",
      "    num_rows: 235\n",
      "})\n",
      "open ì‚­ì œë¨\n",
      "AutoTokenizer ì‚­ì œë¨\n",
      "DataCollatorForSeq2Seq ì‚­ì œë¨\n",
      "Dataset ì‚­ì œë¨\n",
      "DatasetDict ì‚­ì œë¨\n",
      "load_tokenizer_from_local ì‚­ì œë¨\n",
      "boto3 ì‚­ì œë¨\n",
      "pd ì‚­ì œë¨\n",
      "io ì‚­ì œë¨\n",
      "credentials_df ì‚­ì œë¨\n",
      "aws_access_key_id ì‚­ì œë¨\n",
      "aws_secret_access_key ì‚­ì œë¨\n",
      "bucket_name ì‚­ì œë¨\n",
      "file_key ì‚­ì œë¨\n",
      "s3_client ì‚­ì œë¨\n",
      "file_content ì‚­ì œë¨\n",
      "df ì‚­ì œë¨\n",
      "gen_dataset ì‚­ì œë¨\n",
      "gen_train_test_dataset ì‚­ì œë¨\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„° ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "from datasets import Dataset\n",
    "\n",
    "# AWS S3 ì— ì—…ë¡œë“œ í–ˆë˜ ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "# ì—‘ì„¸ìŠ¤í‚¤ê°€ í•„ìš”í•¨\n",
    "try:\n",
    "    credentials_df = pd.read_csv('./ganghyun-dev_accessKeys.csv')\n",
    "\n",
    "    if not credentials_df.empty:\n",
    "        aws_access_key_id = credentials_df['Access key ID'].iloc[0].strip()\n",
    "        aws_secret_access_key = credentials_df['Secret access key'].iloc[0].strip()\n",
    "    else:\n",
    "        print(\"Error: 'aws_credentials.csv' is empty.\")\n",
    "        exit()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'aws_credentials.csv' not found in Drive.\")\n",
    "    print(\"Please create a file named 'aws_credentials.csv' in your Google Drive with your AWS credentials.\")\n",
    "    exit()\n",
    "except KeyError:\n",
    "    print(\"Error: 'Access key ID' or 'Secret access key' column not found in 'aws_credentials.csv'.\")\n",
    "    print(\"Please ensure your CSV file has these columns.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading AWS credentials from CSV: {e}\")\n",
    "    exit()\n",
    "\n",
    "bucket_name = \"dr.hong-s3\"\n",
    "\n",
    "file_key = \"dataset/template_generation_dataset_with_temp_policy.xlsx\"\n",
    "\n",
    "s3_client = boto3.client('s3',\n",
    "                         aws_access_key_id=aws_access_key_id,\n",
    "                         aws_secret_access_key=aws_secret_access_key)\n",
    "\n",
    "try:\n",
    "    file_content = s3_client.get_object(Bucket=bucket_name, Key=file_key)['Body'].read()\n",
    "    print(\"íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ë©”ëª¨ë¦¬ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"S3ì—ì„œ íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ì—‘ì…€ íŒŒì¼ì„ pandas datafrome ìœ¼ë¡œ ë³€í™˜\n",
    "print(\"íŒŒì¼ì„ pandas dataframe ë¡œ ë³€í™˜\")\n",
    "df = pd.read_excel(io.BytesIO(file_content))\n",
    "\n",
    "print(\"ì›ë³¸ ë°ì´í„° ìƒìœ„ 5ê°œ\")\n",
    "print(df.head)\n",
    "print()\n",
    "\n",
    "print(\"Hugging Face Dataset ìœ¼ë¡œ ë³€í™˜\")\n",
    "gen_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "gen_train_test_dataset = gen_dataset.train_test_split(test_size=0.2, shuffle=True)\n",
    "gen_train_dataset = gen_train_test_dataset[\"train\"]\n",
    "gen_test_dataset = gen_train_test_dataset[\"test\"]\n",
    "\n",
    "print(\"ìµœì¢… ë¶„í• ëœ ë°ì´í„° ì…‹\")\n",
    "print(gen_train_dataset)\n",
    "print(gen_test_dataset)\n",
    "\n",
    "cleanup_globals([\"gen_train_dataset\", \"gen_test_dataset\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "execution_state": "idle",
   "id": "ed03d955-3f49-46c1-8431-6353c4f0bef0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T03:49:22.714289Z",
     "iopub.status.busy": "2025-09-05T03:49:22.714059Z",
     "iopub.status.idle": "2025-09-05T03:49:29.437817Z",
     "shell.execute_reply": "2025-09-05T03:49:29.436955Z",
     "shell.execute_reply.started": "2025-09-05T03:49:22.714272Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'MLP-KTLim/llama-3-Korean-Bllossom-8B' ëª¨ë¸ì„ './downloaded_model/MLP-KTLim--llama-3-Korean-Bllossom-8B' ê²½ë¡œì— ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 12 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 15.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cleanup_globals' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m checkpoint = \u001b[33m\"\u001b[39m\u001b[33mMLP-KTLim/llama-3-Korean-Bllossom-8B\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     27\u001b[39m model_path = download_model_snapshot(checkpoint, \u001b[33m\"\u001b[39m\u001b[33m./downloaded_model/\u001b[39m\u001b[33m\"\u001b[39m + checkpoint.replace(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m--\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[43mcleanup_globals\u001b[49m([\u001b[33m\"\u001b[39m\u001b[33mgen_train_dataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mgen_test_dataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel_path\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'cleanup_globals' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "from huggingface_hub import snapshot_download\n",
    "from huggingface_hub.utils import RepositoryNotFoundError\n",
    "\n",
    "def download_model_snapshot(model_id: str, local_dir: str) -> str:\n",
    "    print(f\"'{model_id}' ëª¨ë¸ì„ '{local_dir}' ê²½ë¡œì— ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤...\")\n",
    "    try:\n",
    "        # snapshot_downloadëŠ” ì•Œì•„ì„œ ê¸°ì¡´ íŒŒì¼ì„ ì²´í¬í•˜ê³  í•„ìš”í•œ ê²ƒë§Œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "        model_path = snapshot_download(\n",
    "            repo_id=model_id,\n",
    "            local_dir=local_dir\n",
    "            # resume_download=True, # ê¸°ë³¸ê°’ì´ Trueì´ë¯€ë¡œ ëª…ì‹œí•˜ì§€ ì•Šì•„ë„ ë¨\n",
    "        )\n",
    "        print(\"âœ… ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "        return model_path\n",
    "    except RepositoryNotFoundError:\n",
    "        print(f\"âŒ ì˜¤ë¥˜: ëª¨ë¸ ID '{model_id}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")\n",
    "        return None\n",
    "    \n",
    "# ì‹¤í–‰\n",
    "checkpoint = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"\n",
    "model_path = download_model_snapshot(checkpoint, \"./downloaded_model/\" + checkpoint.replace(\"/\", \"--\"))\n",
    "\n",
    "cleanup_globals([\"gen_train_dataset\", \"gen_test_dataset\", \"model_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "execution_state": "idle",
   "id": "b78aff30-cdf7-43e5-a6be-7317a30af24c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T03:23:16.213063Z",
     "iopub.status.busy": "2025-09-05T03:23:16.212808Z",
     "iopub.status.idle": "2025-09-05T03:23:19.390229Z",
     "shell.execute_reply": "2025-09-05T03:23:19.389608Z",
     "shell.execute_reply.started": "2025-09-05T03:23:16.213046Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying tokenization function to the dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 937/937 [00:00<00:00, 2138.43 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 235/235 [00:00<00:00, 2042.93 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized train dataset features: {'input_ids': List(Value('int32')), 'attention_mask': List(Value('int8')), 'labels': List(Value('int64'))}\n",
      "\n",
      "Tokenized test dataset features: {'input_ids': List(Value('int32')), 'attention_mask': List(Value('int8')), 'labels': List(Value('int64'))}\n",
      "\n",
      "Train dataset size: 937\n",
      "Test dataset size: 235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 937/937 [00:00<00:00, 4468.79 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 235/235 [00:00<00:00, 1530.62 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen_train_dataset ì‚­ì œë¨\n",
      "gen_test_dataset ì‚­ì œë¨\n",
      "AutoTokenizer ì‚­ì œë¨\n",
      "DataCollatorForSeq2Seq ì‚­ì œë¨\n",
      "Dataset ì‚­ì œë¨\n",
      "DatasetDict ì‚­ì œë¨\n",
      "load_tokenizer_from_local ì‚­ì œë¨\n",
      "tokenize_function ì‚­ì œë¨\n",
      "tokenized_gen_train_datasets ì‚­ì œë¨\n",
      "tokenized_gen_eval_datasets ì‚­ì œë¨\n",
      "tokenized_gen_datasets ì‚­ì œë¨\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„° ì…‹ ì „ì²˜ë¦¬\n",
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq\n",
    "from datasets import Dataset, DatasetDict\n",
    "import torch\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë””ìŠ¤í¬ì—ì„œ ë©”ëª¨ë¦¬ë¡œ ë¡œë“œ\n",
    "def load_tokenizer_from_local(model_path: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = load_tokenizer_from_local(model_path)\n",
    "\n",
    "# í† í°í™” í•¨ìˆ˜ ì¬ì •ì˜\n",
    "def tokenize_function(elements):\n",
    "    # ì…ë ¥ ì‹œí€€ìŠ¤ ë§Œë“¤ê¸°\n",
    "    texts = [\n",
    "        f\"{tokenizer.bos_token}user_input: {user_input}\\npolicy: {policy}\\ntemplate: {template}{tokenizer.eos_token}\"\n",
    "        for user_input, policy, template in zip(elements['user_input'], elements['policy'], elements['template'])\n",
    "    ]\n",
    "    # í† í°í™” í•˜ê¸°\n",
    "    tokenized = tokenizer(\n",
    "        texts, \n",
    "        truncation=True, \n",
    "        max_length=8192,\n",
    "        padding=False,  # DataCollator ì—ì„œ padding í•¨\n",
    "    )\n",
    "    \n",
    "    tokenized['labels'] = tokenized['input_ids'].copy() # AutoRegressive\n",
    "    return tokenized\n",
    "\n",
    "# ë°ì´í„°ì…‹ì— í† í°í™” í•¨ìˆ˜ ì ìš©\n",
    "print(\"\\nApplying tokenization function to the dataset...\")\n",
    "tokenized_gen_train_datasets = gen_train_dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True,\n",
    "    remove_columns=gen_train_dataset.column_names\n",
    ")\n",
    "tokenized_gen_eval_datasets = gen_test_dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True,\n",
    "    remove_columns=gen_test_dataset.column_names\n",
    ")\n",
    "\n",
    "tokenized_gen_datasets = DatasetDict({\n",
    "    'train': tokenized_gen_train_datasets,\n",
    "    'eval': tokenized_gen_eval_datasets\n",
    "})\n",
    "\n",
    "print(\"Tokenized train dataset features:\", tokenized_gen_train_datasets.features)\n",
    "print(\"\\nTokenized test dataset features:\", tokenized_gen_eval_datasets.features)\n",
    "print(f\"\\nTrain dataset size: {len(tokenized_gen_train_datasets)}\")\n",
    "print(f\"Test dataset size: {len(tokenized_gen_eval_datasets)}\")\n",
    "\n",
    "tokenized_path = \"./tokenized_datasets/\" + \"MLP-KTLim/llama-3-Korean-Bllossom-8B\".replace(\"/\", \"--\")\n",
    "tokenized_gen_datasets.save_to_disk(tokenized_path)\n",
    "\n",
    "cleanup_globals([\"model_path\", \"tokenizer\", \"tokenized_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "execution_state": "idle",
   "id": "d4cc0790-c7dc-458f-963e-c205617a62b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T05:14:41.880208Z",
     "iopub.status.busy": "2025-09-05T05:14:41.879916Z",
     "iopub.status.idle": "2025-09-05T05:14:42.062878Z",
     "shell.execute_reply": "2025-09-05T05:14:42.062277Z",
     "shell.execute_reply.started": "2025-09-05T05:14:41.880192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ GPU ë©”ëª¨ë¦¬: 0.00 GB\n",
      "í˜„ì¬ ìºì‹œëœ GPU ë©”ëª¨ë¦¬: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# GPUê°€ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸\n",
    "if torch.cuda.is_available():\n",
    "    # í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë©”ëª¨ë¦¬ (ë°”ì´íŠ¸)\n",
    "    allocated_bytes = torch.cuda.memory_allocated(device=0)\n",
    "    # ìºì‹œëœ ë©”ëª¨ë¦¬ (ë°”ì´íŠ¸)\n",
    "    reserved_bytes = torch.cuda.memory_reserved(device=0)\n",
    "\n",
    "    # GB ë‹¨ìœ„ë¡œ ë³€í™˜\n",
    "    gb_factor = 1024 * 1024 * 1024\n",
    "    allocated_gb = allocated_bytes / gb_factor\n",
    "    reserved_gb = reserved_bytes / gb_factor\n",
    "\n",
    "    print(f\"í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ GPU ë©”ëª¨ë¦¬: {allocated_gb:.2f} GB\")\n",
    "    print(f\"í˜„ì¬ ìºì‹œëœ GPU ë©”ëª¨ë¦¬: {reserved_gb:.2f} GB\")\n",
    "\n",
    "else:\n",
    "    print(\"GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "execution_state": "idle",
   "id": "bb1bc9b2-53a6-4675-ae0a-3985bfe58973",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T05:02:09.912195Z",
     "iopub.status.busy": "2025-09-05T05:02:09.911943Z",
     "iopub.status.idle": "2025-09-05T05:02:10.124355Z",
     "shell.execute_reply": "2025-09-05T05:02:10.123673Z",
     "shell.execute_reply.started": "2025-09-05T05:02:09.912180Z"
    }
   },
   "outputs": [],
   "source": [
    "cleanup_globals([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "execution_state": "idle",
   "id": "3e6106df-097d-4706-80be-e75a79f7dd4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T07:04:27.810776Z",
     "iopub.status.busy": "2025-09-05T07:04:27.810472Z",
     "iopub.status.idle": "2025-09-05T07:07:17.590502Z",
     "shell.execute_reply": "2025-09-05T07:07:17.589799Z",
     "shell.execute_reply.started": "2025-09-05T07:04:27.810756Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb0a6163803d45efad601b4d5e4e5273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA ì–´ëŒ‘í„° ì´ˆê¸°í™” ì¤‘...\n",
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n",
      "open ì‚­ì œë¨\n",
      "AutoTokenizer ì‚­ì œë¨\n",
      "AutoModelForCausalLM ì‚­ì œë¨\n",
      "BitsAndBytesConfig ì‚­ì œë¨\n",
      "LoraConfig ì‚­ì œë¨\n",
      "get_peft_model ì‚­ì œë¨\n",
      "prepare_model_for_kbit_training ì‚­ì œë¨\n",
      "model_path ì‚­ì œë¨\n",
      "load_model_and_tokenizer_from_local ì‚­ì œë¨\n",
      "lora_config ì‚­ì œë¨\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "model_path = \"/mnt/custom-file-systems/s3/shared/downloaded_model/MLP-KTLim--llama-3-Korean-Bllossom-8B\"\n",
    "\n",
    "def load_model_and_tokenizer_from_local(model_path: str):\n",
    "    # load_in_8bit=True ì˜µì…˜ìœ¼ë¡œ 8ë¹„íŠ¸ ì–‘ìí™”ë¥¼ í™œì„±í™”í•©ë‹ˆë‹¤.\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        llm_int8_threshold=6.0, # ì–‘ìí™”ì—ì„œ ì œì™¸í•  ì•„ì›ƒë¼ì´ì–´ ì„ê³„ê°’ ì„¤ì •\n",
    "        llm_int8_has_fp16_weight=False, # ëª¨ë¸ì˜ ì¼ë¶€ë¥¼ FP16ìœ¼ë¡œ ìœ ì§€í• ì§€ ì„¤ì •\n",
    "        llm_int8_skip_modules=None, # 8ë¹„íŠ¸ ì–‘ìí™”ì—ì„œ ì œì™¸í•  ëª¨ë“ˆ ëª©ë¡\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_config,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        dtype=torch.float16\n",
    "    )\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer_from_local(model_path)\n",
    "\n",
    "# ëª¨ë¸ì„ kbit í•™ìŠµì„ ìœ„í•´ ì¤€ë¹„\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# LoRA ì„¤ì •\n",
    "print(\"LoRA ì–´ëŒ‘í„° ì´ˆê¸°í™” ì¤‘...\")\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # í•™ìŠµëŠ¥ë ¥ì¹˜, ë†’ì´ë©´ í•™ìŠµ ì„±ëŠ¥ì´ ì˜¬ë¼ê°€ì§€ë§Œ ë©”ëª¨ë¦¬ì™€ ì†ë„ì—ì„œ ì†í•´\n",
    "    lora_alpha=16, # ê°€ì¤‘ì¹˜ ìŠ¤ì¼€ì¼ë§, ë³´í†µ r ì˜ ë‘ë°°\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # íƒ€ê²Ÿ ëª¨ë“ˆ ì¶”ê°€\n",
    "    # target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # ë” ì •ë°€í•œ íŒŒì¸íŠœë‹ì„ í•˜ë ¤ë©´ íƒ€ê²Ÿ ì¶”ê°€\n",
    "    lora_dropout=0.05, # ê³¼ì í•©ì´ ë°œìƒí•˜ë©´ ë†’ì—¬ë³¼ ìˆ˜ ìˆìŒ\n",
    "    bias=\"none\", # none ì´ ì¼ë°˜ì ì„, ë‹¤ë¥¸ ì˜µì…˜ì€ ì˜í–¥ì´ ê±°ì˜ ì—†ìŒ\n",
    "    task_type=\"CAUSAL_LM\", # ëª¨ë¸ì— ë§ì¶°ì„œ\n",
    "    # inference_mode=False  # í•™ìŠµ ëª¨ë“œ í™œì„±í™” ê¸°ë³¸ê°’\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ì„ PEFT ëª¨ë¸ë¡œ ë³€í™˜\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "cleanup_globals([\"tokenizer\", \"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "execution_state": "idle",
   "id": "36bebec0-8eb3-4945-9eb5-bca0b9690a13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T07:07:17.591863Z",
     "iopub.status.busy": "2025-09-05T07:07:17.591488Z",
     "iopub.status.idle": "2025-09-05T07:07:18.923720Z",
     "shell.execute_reply": "2025-09-05T07:07:18.923051Z",
     "shell.execute_reply.started": "2025-09-05T07:07:17.591845Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "tokenized_path = \"./tokenized_datasets/\" + \"MLP-KTLim/llama-3-Korean-Bllossom-8B\".replace(\"/\", \"--\")\n",
    "tokenized_gen_datasets = load_from_disk(tokenized_path)\n",
    "tokenized_gen_train_datasets = tokenized_gen_datasets['train']\n",
    "tokenized_gen_eval_datasets = tokenized_gen_datasets['eval']\n",
    "\n",
    "# DataCollator ì •ì˜ - íŒ¨ë”©ê³¼ ë¼ë²¨ ì²˜ë¦¬ë¥¼ ìœ„í•´ DataCollatorForSeq2Seq ì‚¬ìš©\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "execution_state": "idle",
   "id": "8aecd728-5c00-4680-8e74-00dc549e7dd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T07:07:18.924736Z",
     "iopub.status.busy": "2025-09-05T07:07:18.924446Z",
     "iopub.status.idle": "2025-09-05T07:46:42.827814Z",
     "shell.execute_reply": "2025-09-05T07:46:42.827073Z",
     "shell.execute_reply.started": "2025-09-05T07:07:18.924720Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbill291104\u001b[0m (\u001b[33mdr-hong\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for wandb.init()..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/custom-file-systems/s3/shared/wandb/run-20250905_070720-tp7c04nc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dr-hong/dr-hong/runs/tp7c04nc' target=\"_blank\">llama-3-Korean-Bllossom-8B-v1</a></strong> to <a href='https://wandb.ai/dr-hong/dr-hong' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dr-hong/dr-hong' target=\"_blank\">https://wandb.ai/dr-hong/dr-hong</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dr-hong/dr-hong/runs/tp7c04nc' target=\"_blank\">https://wandb.ai/dr-hong/dr-hong/runs/tp7c04nc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/transformers/training_args.py:1818: FutureWarning: `torchdynamo` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use `torch_compile_backend` instead\n",
      "  warnings.warn(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128001}.\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "Casting fp32 inputs back to torch.bfloat16 for flash-attn compatibility.\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [90/90 37:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.851100</td>\n",
       "      <td>0.712596</td>\n",
       "      <td>0.754690</td>\n",
       "      <td>0.691437</td>\n",
       "      <td>0.754690</td>\n",
       "      <td>0.433456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.686000</td>\n",
       "      <td>0.612044</td>\n",
       "      <td>0.915666</td>\n",
       "      <td>0.883394</td>\n",
       "      <td>0.915666</td>\n",
       "      <td>0.592565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.628600</td>\n",
       "      <td>0.549853</td>\n",
       "      <td>0.888473</td>\n",
       "      <td>0.857504</td>\n",
       "      <td>0.888473</td>\n",
       "      <td>0.677233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.567500</td>\n",
       "      <td>0.512030</td>\n",
       "      <td>0.908745</td>\n",
       "      <td>0.866209</td>\n",
       "      <td>0.908745</td>\n",
       "      <td>0.731451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.482800</td>\n",
       "      <td>0.486619</td>\n",
       "      <td>0.928802</td>\n",
       "      <td>0.874780</td>\n",
       "      <td>0.928802</td>\n",
       "      <td>0.619834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.514900</td>\n",
       "      <td>0.468702</td>\n",
       "      <td>0.898214</td>\n",
       "      <td>0.853021</td>\n",
       "      <td>0.898214</td>\n",
       "      <td>0.629576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.438300</td>\n",
       "      <td>0.457031</td>\n",
       "      <td>0.884503</td>\n",
       "      <td>0.776887</td>\n",
       "      <td>0.884503</td>\n",
       "      <td>0.743210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.477500</td>\n",
       "      <td>0.448802</td>\n",
       "      <td>0.916825</td>\n",
       "      <td>0.872147</td>\n",
       "      <td>0.916825</td>\n",
       "      <td>0.709512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.443300</td>\n",
       "      <td>0.441495</td>\n",
       "      <td>0.942280</td>\n",
       "      <td>0.912324</td>\n",
       "      <td>0.942280</td>\n",
       "      <td>0.764197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.442300</td>\n",
       "      <td>0.436455</td>\n",
       "      <td>0.931395</td>\n",
       "      <td>0.866178</td>\n",
       "      <td>0.931395</td>\n",
       "      <td>0.664749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.423800</td>\n",
       "      <td>0.433619</td>\n",
       "      <td>0.901792</td>\n",
       "      <td>0.871921</td>\n",
       "      <td>0.901792</td>\n",
       "      <td>0.735143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.460900</td>\n",
       "      <td>0.430128</td>\n",
       "      <td>0.876112</td>\n",
       "      <td>0.863254</td>\n",
       "      <td>0.876112</td>\n",
       "      <td>0.738583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.421500</td>\n",
       "      <td>0.430017</td>\n",
       "      <td>0.857937</td>\n",
       "      <td>0.774208</td>\n",
       "      <td>0.835714</td>\n",
       "      <td>0.819977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='59' max='59' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [59/59 00:46]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>â–â–„â–…â–†â–„â–…â–‡â–†â–‡â–…â–†â–‡â–ˆâ–†</td></tr><tr><td>eval/loss</td><td>â–ˆâ–†â–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–</td></tr><tr><td>eval/rouge1</td><td>â–â–‡â–†â–‡â–‡â–†â–†â–‡â–ˆâ–ˆâ–†â–†â–…â–‡</td></tr><tr><td>eval/rouge2</td><td>â–â–‡â–†â–‡â–‡â–†â–„â–‡â–ˆâ–‡â–‡â–†â–„â–‡</td></tr><tr><td>eval/rougeL</td><td>â–â–‡â–†â–‡â–‡â–†â–†â–‡â–ˆâ–ˆâ–†â–†â–„â–‡</td></tr><tr><td>eval/runtime</td><td>â–‚â–â–â–â–â–â–â–â–‚â–â–â–â–â–ˆ</td></tr><tr><td>eval/samples_per_second</td><td>â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–</td></tr><tr><td>eval/steps_per_second</td><td>â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–</td></tr><tr><td>train/epoch</td><td>â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>+3</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>0.71261</td></tr><tr><td>eval/loss</td><td>0.42933</td></tr><tr><td>eval/rouge1</td><td>0.90319</td></tr><tr><td>eval/rouge2</td><td>0.89441</td></tr><tr><td>eval/rougeL</td><td>0.90319</td></tr><tr><td>eval/runtime</td><td>47.9964</td></tr><tr><td>eval/samples_per_second</td><td>4.896</td></tr><tr><td>eval/steps_per_second</td><td>1.229</td></tr><tr><td>total_flos</td><td>5.449679907087974e+16</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>+8</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">llama-3-Korean-Bllossom-8B-v1</strong> at: <a href='https://wandb.ai/dr-hong/dr-hong/runs/tp7c04nc' target=\"_blank\">https://wandb.ai/dr-hong/dr-hong/runs/tp7c04nc</a><br> View project at: <a href='https://wandb.ai/dr-hong/dr-hong' target=\"_blank\">https://wandb.ai/dr-hong/dr-hong</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250905_070720-tp7c04nc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import wandb\n",
    "import evaluate\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# wandb ì´ˆê¸°í™”\n",
    "wandb.init( # ê¸°ë¡ì„ ìœ„í•œ ì •ë³´ì´ë©° í•™ìŠµì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŒ\n",
    "    entity=\"dr-hong\",\n",
    "    project=\"dr-hong\",\n",
    "    name=\"llama-3-Korean-Bllossom-8B-v1\",\n",
    "    config={\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"epochs\": 3,\n",
    "        \"batch_size\": 4,\n",
    "        \"model_name\": \"MLP-KTLim/llama-3-Korean-Bllossom-8B\",\n",
    "        \"lora_r\": 8,\n",
    "        \"lora_alpha\": 16,\n",
    "    }\n",
    ")\n",
    "\n",
    "# compute_metrics í•¨ìˆ˜ ì •ì˜. llama3 ì— ì í•©í•œ í‰ê°€ì§€í‘œ ì„ íƒ: ROUGE, BLEU\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "def compute_metrics(eval_preds, compute_result=False):\n",
    "    predictions, labels = eval_preds.predictions, eval_preds.label_ids\n",
    "    predictions = predictions.argmax(axis=-1)  # logits â†’ token ids\n",
    "\n",
    "    labels[labels == -100] = tokenizer.pad_token_id\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    rouge_result = rouge.compute(\n",
    "        predictions=decoded_preds, \n",
    "        references=decoded_labels\n",
    "    )\n",
    "    \n",
    "    bleu_result = bleu.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=[[label] for label in decoded_labels]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"rouge1\": rouge_result[\"rouge1\"],\n",
    "        \"rouge2\": rouge_result[\"rouge2\"],\n",
    "        \"rougeL\": rouge_result[\"rougeL\"],\n",
    "        \"bleu\": bleu_result[\"bleu\"]\n",
    "    }\n",
    "\n",
    "# í•™ìŠµ ì¸ì ì •ì˜\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/\" + \"MLP-KTLim/llama-3-Korean-Bllossom-8B\".replace(\"/\", \"--\"),\n",
    "    # overwrite_output_dir=True, # False ê¸°ë³¸ê°’ì´ë©´ ìë™ìœ¼ë¡œ ì¤‘ë‹¨ë˜ì—ˆë˜ í•™ìŠµì„ ì´ì–´ì„œ í•  ìˆ˜ ìˆìŒ, True ë©´ ì²˜ìŒë¶€í„° ë‹¤ì‹œ\n",
    "    do_train=True, # í•™ìŠµ ë‹¨ê³„ë¥¼ ê±´ë„ˆë›¸ ìˆ˜ë„ ìˆìŒ\n",
    "    do_eval=True, # í‰ê°€ë¥¼ ë”°ë¡œ ì§„í–‰í•  ìˆ˜ ìˆìŒ\n",
    "    # do_predict=False, # ì¶”ë¡ ë‹¨ê³„ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì•ˆí•¨\n",
    "    eval_strategy=\"steps\", # ì¼ì • step ë§ˆë‹¤ í‰ê°€\n",
    "    # prediction_loss_only=False, # True ë©´ í‰ê°€ì™€ ì¶”ë¡  ì‘ì—…ì—ì„œ loss ë§Œ ë°˜í™˜í•¨\n",
    "    per_device_train_batch_size=4, # í•™ìŠµ ë°°ì¹˜ ì‚¬ì´ì¦ˆ, ê¸°ë³¸ì€ 8ì¸ë° ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ì¤„ì„\n",
    "    per_device_eval_batch_size=4, # í‰ê°€ ë°°ì¹˜ ì‚¬ì´ì¦ˆ, ê¸°ë³¸ì€ 8ì¸ë° ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ì¤„ì„\n",
    "    gradient_accumulation_steps=8, # ì¤„ì¸ ë°°ì¹˜ì‚¬ì´ì¦ˆë¥¼ ë³´ì™„í•˜ëŠ” ë°°ìˆ˜, ì‹¤ì œ íš¨ê³¼ëŠ” ë°°ì¹˜ì‚¬ì´ì¦ˆì™€ ì´ ê°’ì„ ê³±í•œ í¬ê¸°ë¥¼ ë°°ì¹˜ì‚¬ì´ì¦ˆë¡œ í•˜ëŠ” ê²ƒê³¼ ë™ì¼í•¨\n",
    "    eval_accumulation_steps=64, # í‰ê°€ ë‹¨ê³„ì—ì„œ GPU ë©”ëª¨ë¦¬ ë¬¸ì œê°€ ë°œìƒí•˜ë©´ ì„¤ì •, í‚¤ìš°ë©´ CPU ë©”ëª¨ë¦¬ë¡œ ë” ìì£¼ ê²°ê³¼ë¥¼ ì˜®ê¹€\n",
    "    eval_delay=30, # í•™ìŠµ ì´ˆê¸° ë‹¨ê³„ì—ì„œ í‰ê°€ë¥¼ ê±´ë„ˆëœ€, ì˜ë¯¸ì—†ëŠ” ê³„ì‚°ì„ ì¤„ì„. í•™ìŠµì…‹ 937ê°œ / (ì‹¤ì§ˆì  ë°°ì¹˜ ì‚¬ì´ì¦ˆ 4 * 8) ~= 30 -> 1epoch ì •ë„ëŠ” í‰ê°€ë¥¼ ê±´ë„ˆëœ€\n",
    "    torch_empty_cache_steps=30, # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ëŠ” ì‹œì . ì•ˆì „í•˜ê²Œ eval_step ê³¼ ë™ì¼í•œ ê°’ìœ¼ë¡œ\n",
    "    learning_rate=2e-4, # AdamW ì˜µí‹°ë§ˆì´ì €ë¥¼ ìœ„í•œ ì´ˆê¸° í•™ìŠµë¥ . LoRA ë¥¼ ì“´ë‹¤ë©´ 2e-4 ê°€ ì¢‹ìŒ\n",
    "    weight_decay=0.01, # ì •ê·œí™” ê¸°ëŠ¥. ì¼ë°˜ì ìœ¼ë¡œ 0.01 ì‚¬ìš©. ê³¼ì í•©ì´ë©´ ë†’ì„\n",
    "    # adam_beta1, adam_beta2, adam_epsilon, max_grad_norm ì€ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©\n",
    "    # num_train_epochs=3, # í•™ìŠµ íšŸìˆ˜. ê¸°ë³¸ 3\n",
    "    # max_steps ì‚¬ìš© ì•ˆí•¨\n",
    "    # lr_scheduler_type, lr_scheduler_kwargs ëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©\n",
    "    warmup_ratio=0.1, # 10% ë¥¼ ì›œì—… ë‹¨ê³„ë¡œ ì„¤ì •\n",
    "    # warmup_step=9 # 3 ì—í¬í¬ * 30 ìŠ¤í… * 0.1 = 9\n",
    "    # log_level=\"passive\", # ë¡œê·¸ ë ˆë²¨ ê¸°ë³¸ê°’ ì‚¬ìš©. warning\n",
    "    # log_level_replica, log_on_each_node ëŠ” ë¶„ì‚° í•™ìŠµìš©\n",
    "    logging_dir=\"./logs/\" + \"MLP-KTLim/llama-3-Korean-Bllossom-8B\".replace(\"/\", \"--\"), # TensorBoard ë¡œ ë¡œê·¸ í™•ì¸ ê°€ëŠ¥\n",
    "    logging_strategy=\"steps\", # epoch ê°€ ì–¼ë§ˆ ì—†ì–´ì„œ steps ì‚¬ìš©\n",
    "    # logging_first_step=False\n",
    "    logging_steps=5, # 1 epoch ì™€ ë™ì¼í•˜ê²Œ\n",
    "    # logging_nan_inf_filter=True # ê¸°ë³¸ True. nan, inf ë¥¼ ë¬´ì‹œí•˜ê³  ë¡œê·¸ë¥¼ ê¹”ë”í•˜ê²Œ\n",
    "    save_strategy=\"steps\", # logging_strategy ì™€ ë™ì¼í•˜ê²Œ\n",
    "    save_steps=15, # epoch ì „ëµì´ë©´ ë¬´ì‹œë¨\n",
    "    save_total_limit=1, # ì €ì¥ë˜ëŠ” ì²´í¬í¬ì¸íŠ¸ í•œê°œë§Œ ìœ ì§€\n",
    "    # save_safetensors=True # ê¸°ë³¸ True. sagetensor í˜•ì‹ìœ¼ë¡œ ì €ì¥\n",
    "    # save_on_each_node ëŠ” ë¶„ì‚° í•™ìŠµì—ì„œ ì‚¬ìš©\n",
    "    # save_only_model=False # ì´ì–´ì„œ í•™ìŠµì´ ê°€ëŠ¥í•˜ë„ë¡ ê¸°ë³¸ê°’ False ë¡œ ìœ ì§€\n",
    "    # restore_callback_states_from_checkpoint ì€ ì½œë°± ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë©´ ì„¤ì • ê°€ëŠ¥\n",
    "    # use_cpu=False # GPU ì‚¬ìš©í•˜ë¯€ë¡œ ê¸°ë³¸ê°’ False ìœ ì§€\n",
    "    # seed, data_seed ëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©\n",
    "    # use_ipex=False # NVIDIA GPU ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ê¸°ë³¸ê°’ False ì‚¬ìš©\n",
    "    bf16=True, # ìµœì‹  GPU ë¼ë©´ bf16, êµ¬í˜•ì´ë¼ë©´ fp16 ì‚¬ìš©\n",
    "    # half_precision_backend=\"auto\" # í˜¼í•© ì •ë°€ë„ ê¸°ëŠ¥ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬, ê¸°ë³¸ê°’ auto ì‚¬ìš©\n",
    "    tf32=True, # ml.g5.xlarge ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì‘ì—…í•˜ë¯€ë¡œ ì‚¬ìš©ê°€ëŠ¥í•œ ì˜µì…˜\n",
    "    # local_rank=-1 # ë¶„ì‚° í•™ìŠµì´ ì•„ë‹ˆë¯€ë¡œ ê¸°ë³¸ê°’ -1\n",
    "    # ddp_backend=\"nccl\" # ë¶„ì‚° í•™ìŠµì´ë©´ ì‚¬ìš©\n",
    "    eval_steps=5, # eval_strategy ê°€ steps ë©´ logging_steps ì™€ ë™ì¼í•œ ê°’ì´ ê¸°ë³¸\n",
    "    dataloader_num_workers=4, # ml.g5.xlarge ëŠ” cpu ì½”ì–´ìˆ˜ê°€ 4ê°œ. ë°ì´í„°ë¡œë”ê°€ ë³‘ë ¬ì²˜ë¦¬ ê°€ëŠ¥\n",
    "    disable_tqdm=False, # ì§„í–‰ë„ ì‹œê°í™” ê¸°ëŠ¥ ì‚¬ìš©\n",
    "    # remove_unused_columns=True, # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ê¸°ë³¸ê°’ True ì‚¬ìš©\n",
    "    # label_names ëŠ” label ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ê¸°ë³¸ì´ê³  ê·¸ë ‡ê²Œ ì „ì²˜ë¦¬ í–ˆìŒ\n",
    "    load_best_model_at_end=True, # save_total_limit=1 ê³¼ ì¡°í•©ì´ ì¢‹ìŒ\n",
    "    metric_for_best_model=\"eval_loss\", # ê°€ì¥ ì¢‹ì€ ëª¨ë¸ ì„ íƒ ê¸°ì¤€: eval loss\n",
    "    greater_is_better=False, # loss ëŠ” ë‚®ì€ê²Œ ì¢‹ì€ ê²°ê³¼ì„\n",
    "    # ignore_data_skip=False, # ì´ì–´ì„œ í•™ìŠµ í•˜ê¸°ìœ„í•´ ê¸°ë³¸ê°’ False ì‚¬ìš©\n",
    "    # fsdp, fsdp_config, accelerator_config, parallelism_config ëŠ” ë¶„ì‚° í•™ìŠµì—ì„œ ì‚¬ìš©\n",
    "    # label_smoothing_factor=0.1 # ê³¼ì í•©ì´ ë°œìƒí•˜ë©´ ì„¤ì •\n",
    "    # optim, optim_args ëŠ” ê¸°ë³¸ì¸ AdamW ë¥¼ ì‚¬ìš©\n",
    "    group_by_length=True, # Dynamic Padding ì„ í–ˆë‹¤ë©´ ë©”ëª¨ë¦¬ ì ˆì•½ ê°€ëŠ¥\n",
    "    report_to=\"all\", # ëª¨ë“  ì‚¬ìš© ê°€ëŠ¥í•œ í”Œë«í¼ì— ë³´ê³ , ì§€ê¸ˆì€ wandb\n",
    "    # ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers ëŠ” ë¶„ì‚° í™˜ê²½ì—ì„œ ì‚¬ìš©\n",
    "    # dataloader_pin_memory=True, # GPU -> CPU ë©”ëª¨ë¦¬ ì´ë™ì´ ë¹¨ë¼ì§, ê¸°ë³¸ê°’ True ì‚¬ìš©\n",
    "    dataloader_persistent_workers=True, # CPU RAM ì´ ë¶€ì¡±í•˜ë©´ False ë¡œ\n",
    "    # dataloader_prefetch_factor=2, # CPU ë©”ëª¨ë¦¬ê°€ ì—¬ìœ ë¡­ë‹¤ë©´ 2 ì´ìƒìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥\n",
    "    # skip_memory_metrics=True # ë©”ëª¨ë¦¬ ë¬¸ì œì˜ ì›ì¸ì„ ì°¾ì•„ì•¼ í•œë‹¤ë©´ False ì‚¬ìš©\n",
    "    # push_to_hub=False # ì—…ë¡œë“œ ì•ˆí•¨\n",
    "    resume_from_checkpoint=\"./results/\" + \"MLP-KTLim/llama-3-Korean-Bllossom-8B\".replace(\"/\", \"--\"), # í•™ìŠµ ì¬ê°œ ê°€ëŠ¥\n",
    "    gradient_checkpointing=True, # ë©”ëª¨ë¦¬ ë¬¸ì œ ë°©ì§€\n",
    "    # eval_do_concat_batches=True, # í‰ê°€ì§€í‘œ ê³„ì‚° ì†ë„ë¥¼ ìœ„í•´ ê¸°ë³¸ê°’ True ì‚¬ìš©\n",
    "    auto_find_batch_size=True, # ë‹¤ë¥¸ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì„¤ì •ì„ ë®ì–´ì”€. ìë™ìœ¼ë¡œ ì‹¤í–‰ê°€ëŠ¥í•œ ìµœì ì˜ ë°°ì¹˜ í¬ê¸°ë¥¼ ì°¾ì•„ì¤Œ. ì²˜ìŒ í•œë²ˆì€ ì´ ê¸°ëŠ¥ì„ ì‚¬ìš©í•´ì„œ ë°°ì¹˜ í¬ê¸°ë¥¼ ì°¾ëŠ”ê²Œ ì¢‹ìŒ\n",
    "    # full_determinism=False, # ë‚œìˆ˜ ê³ ì • ê¸°ëŠ¥. ì—°êµ¬ì— ì‚¬ìš© ê°€ëŠ¥\n",
    "    torchdynamo=\"inductor\", # í•™ìŠµ ì†ë„ ì˜¬ë ¤ì£¼ëŠ” PyTorch ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "    torch_compile=True, # ëª¨ë¸ ìµœì í™” ê¸°ëŠ¥\n",
    "    torch_compile_backend=\"inductor\", # ì»´íŒŒì¼ í•´ì£¼ëŠ” ë°±ì—”ë“œ ì§€ì •\n",
    "    torch_compile_mode=\"default\",\n",
    "    neftune_noise_alpha=5.0, # llama 3 ê°™ì€ ëª…ë ¹ì–´ íŒŒì¸íŠœë‹ì— íš¨ê³¼ì ì¸ ê¸°ë²•\n",
    "    batch_eval_metrics=True, # í‰ê°€ë‹¨ê³„ ë©”ëª¨ë¦¬ ì ˆì•½\n",
    "    # eval_on_start=True, # í‰ê°€ë‹¨ê³„ ë””ë²„ê¹…ì„ ë¹ ë¥´ê²Œ í•´ë³¼ë ¤ë©´ ì‚¬ìš©\n",
    ")\n",
    "\n",
    "# Trainer ì„¤ì •\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_gen_train_datasets,\n",
    "    eval_dataset=tokenized_gen_eval_datasets,\n",
    "    processing_class=tokenizer,\n",
    "    # model_init ëŠ” í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹í•  ë•Œ ì‚¬ìš©\n",
    "    compute_metrics=compute_metrics,\n",
    "    # optimizers ëŠ” ê¸°ë³¸ AdamW ì‚¬ìš©\n",
    ")\n",
    "\n",
    "try:\n",
    "    train_output = trainer.train()\n",
    "    \n",
    "    eval_results = trainer.evaluate()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "    raise e\n",
    "\n",
    "# ìµœì¢… ëª¨ë¸ ì €ì¥\n",
    "model.save_pretrained(\"./finetuned_model/\" + \"MLP-KTLim/llama-3-Korean-Bllossom-8B\".replace(\"/\", \"--\"))\n",
    "\n",
    "# wandb ì¢…ë£Œ\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "execution_state": "idle",
   "id": "7818c3c3-222c-4e23-a3b1-9825d77df17b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T08:26:33.551647Z",
     "iopub.status.busy": "2025-09-05T08:26:33.551248Z",
     "iopub.status.idle": "2025-09-05T08:29:10.325298Z",
     "shell.execute_reply": "2025-09-05T08:29:10.324641Z",
     "shell.execute_reply.started": "2025-09-05T08:26:33.551630Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8a983d04344b9192d621da2eb806cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"title\": \"íšŒì‚¬ì†Œê°œì„œ ë°œì†¡\", \"text\": \"ì•ˆë…•í•˜ì„¸ìš” #{ìˆ˜ì‹ ìëª…}ë‹˜,\\n\\n#{íšŒì‚¬ëª…}ì€ #{ì—…ì¢…} ë¶„ì•¼ì—ì„œ í™œë™í•˜ëŠ” #{íšŒì‚¬ëª…}ì…ë‹ˆë‹¤.\\n\\nâ–¶ íšŒì‚¬ëª… : #{íšŒì‚¬ëª…}\\nâ–¶ ì—…ì¢… : #{ì—…ì¢…}\\nâ–¶ ì—°ë½ì²˜ : #{ì—°ë½ì²˜}\\n\\nê°ì‚¬í•©ë‹ˆë‹¤.\", \"button_name\": \"ìì„¸íˆ ë³´ê¸°\"}ëª©ì : ê³ ê°ì—ê²Œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì´ ëª©ì ì´ë©°, ì•Œë¦¼í†¡ìœ¼ë¡œ ë°œì†¡ ê°€ëŠ¥í•œ ì •ë³´ì„± ë©”ì‹œì§€ì…ë‹ˆë‹¤. ì •ë³´í†µì‹ ë§ë²•ê³¼ ì¹´ì¹´ì˜¤í†¡ ë‚´ë¶€ ê¸°ì¤€ì— ë”°ë¼ ì‹¬ì‚¬ ì§„í–‰ë˜ê³  ìŠ¹ì¸ëœ ì•Œë¦¼í†¡ í…œ\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# ë² ì´ìŠ¤ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "base_model_path = \"/mnt/custom-file-systems/s3/shared/downloaded_model/MLP-KTLim--llama-3-Korean-Bllossom-8B\"\n",
    "lora_adapter_path = \"./finetuned_model/\" + \"MLP-KTLim/llama-3-Korean-Bllossom-8B\".replace(\"/\", \"--\")\n",
    "\n",
    "# 8ë¹„íŠ¸ ì–‘ìí™” ì„¤ì •\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0\n",
    ")\n",
    "\n",
    "# ì¶”ë¡ ìš© ëª¨ë¸ ë¡œë“œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë²„ì „)\n",
    "def load_inference_model_and_tokenizer(base_model_path, lora_adapter_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_path,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(base_model, lora_adapter_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "base_model, tokenizer = load_inference_model_and_tokenizer(base_model_path, lora_adapter_path)\n",
    "\n",
    "# ì¶”ë¡  í•¨ìˆ˜\n",
    "def generate_template(user_input, policy):\n",
    "    prompt = f\"\"\"ì‚¬ìš©ì ìš”ì²­: {user_input}\n",
    "ì •ì±…: {policy}\n",
    "í…œí”Œë¦¿:\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True).to(base_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = base_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            temperature=0.3,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response.split(\"í…œí”Œë¦¿:\")[-1].strip()\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "user_request = \"íšŒì‚¬ì†Œê°œì„œ ë°œì†¡ í…œí”Œë¦¿ ì œì‘ ë¶€íƒë“œë ¤ìš”\"\n",
    "policy_text = \"ì •ë³´ì„± ë©”ì‹œì§€ë€ ì •ë³´í†µì‹ ë§ë²• ì•ˆë‚´ì„œì— 'ì˜ë¦¬ëª©ì  ê´‘ê³ ì„± ì •ë³´ì˜ ì˜ˆì™¸'ì— í•´ë‹¹í•˜ëŠ” ë©”ì‹œì§€ì…ë‹ˆë‹¤.\"\n",
    "\n",
    "result = generate_template(user_request, policy_text)\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dr-hong-pr",
   "language": "python",
   "name": "dr-hong-pr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
