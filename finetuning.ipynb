{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "execution_state": "idle",
   "id": "1de26902-96d9-4f3b-8458-054fbe68b00b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T07:03:49.947770Z",
     "iopub.status.busy": "2025-09-05T07:03:49.947570Z",
     "iopub.status.idle": "2025-09-05T07:03:54.969285Z",
     "shell.execute_reply": "2025-09-05T07:03:54.968746Z",
     "shell.execute_reply.started": "2025-09-05T07:03:49.947755Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "def cleanup_globals(vars_to_keep: list):\n",
    "    global_vars = list(globals().keys())\n",
    "    protected_vars = ['In', 'Out', 'get_ipython', 'exit', 'quit', 'gc', 'torch', 'cleanup_globals']\n",
    "\n",
    "    for var in global_vars:\n",
    "        if var not in vars_to_keep and not var.startswith('_') and var not in protected_vars:\n",
    "            try:\n",
    "                del globals()[var]\n",
    "                print(f\"{var} 삭제됨\")\n",
    "            except:\n",
    "                continue\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "execution_state": "idle",
   "id": "68e91c4e-1ab2-4aa1-a583-b759227a7dc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T03:23:01.178461Z",
     "iopub.status.busy": "2025-09-05T03:23:01.178236Z",
     "iopub.status.idle": "2025-09-05T03:23:03.105600Z",
     "shell.execute_reply": "2025-09-05T03:23:03.104955Z",
     "shell.execute_reply.started": "2025-09-05T03:23:01.178444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일을 성공적으로 메모리로 불러왔습니다.\n",
      "파일을 pandas dataframe 로 변환\n",
      "원본 데이터 상위 5개\n",
      "<bound method NDFrame.head of                                                template  \\\n",
      "0     {\"title\": \"회사소개서 발송\", \"text\": \"안녕하세요 #{수신자명}님,...   \n",
      "1     {\"title\": \"서비스 소개서 발송\", \"text\": \"안녕하세요 #{수신자명}...   \n",
      "2     {\"title\": \"(전용) 강의 일정 안내 / 화케터\", \"text\": \"안녕하세...   \n",
      "3     {\"title\": \"(공용) 후기 작성 요청_이미지형_01\", \"text\": \"[템...   \n",
      "4     {\"title\": \"(공용) 인보이스 알림_이미지형_01\", \"text\": \"■ #...   \n",
      "...                                                 ...   \n",
      "1167  {\"title\": \"재입고 알림\", \"text\": \"안녕하세요 #{수신자명}님,\\n...   \n",
      "1168  {\"title\": \"사전 구매 예약\", \"text\": \"안녕하세요 #{수신자명}님,...   \n",
      "1169  {\"title\": \"정기구독 결제 안내\", \"text\": \"안녕하세요 #{수신자명}...   \n",
      "1170  {\"title\": \"자동 결제 정보 등록 완료\", \"text\": \"안녕하세요 #{수...   \n",
      "1171  {\"title\": \"자동 결제 안내\", \"text\": \"안녕하세요 #{수신자명}님,...   \n",
      "\n",
      "                                user_input  \\\n",
      "0             안녕하세요, 회사소개서 발송 템플릿 제작 부탁드려요   \n",
      "1                           서비스 소개서 발송 템플릿   \n",
      "2        (전용) 강의 일정 안내 / 화케터용 알림톡을 만들고 싶어요   \n",
      "3             (공용) 후기 작성 요청_이미지형_01 안내 메시지   \n",
      "4       (공용) 인보이스 알림_이미지형_01용 알림톡을 만들고 싶어요   \n",
      "...                                    ...   \n",
      "1167      고객에게 보낼 재입고 알림 메시지를 작성해주실 수 있나요?   \n",
      "1168                       사전 구매 예약 알림톡 제작   \n",
      "1169  정기구독 결제 안내 알림 메시지가 필요한데 도움 주실 수 있나요?   \n",
      "1170    안녕하세요! 자동 결제 정보 등록 완료 관련 템플릿이 필요해요   \n",
      "1171                       자동 결제 안내 템플릿 생성   \n",
      "\n",
      "                                                 policy  \n",
      "0     정보성 메시지란 정보통신망법 안내서에 '영리목적 광고성 정보의 예외'에 해당하는 메...  \n",
      "1     정보성 메시지란 정보통신망법 안내서에 '영리목적 광고성 정보의 예외'에 해당하는 메...  \n",
      "2     정보성 메시지란 정보통신망법 안내서에 '영리목적 광고성 정보의 예외'에 해당하는 메...  \n",
      "3     정보성 메시지란 정보통신망법 안내서에 '영리목적 광고성 정보의 예외'에 해당하는 메...  \n",
      "4     정보성 메시지란 정보통신망법 안내서에 '영리목적 광고성 정보의 예외'에 해당하는 메...  \n",
      "...                                                 ...  \n",
      "1167  정보성 메시지란 정보통신망법 안내서에 '영리목적 광고성 정보의 예외'에 해당하는 메...  \n",
      "1168  정보성 메시지란 정보통신망법 안내서에 '영리목적 광고성 정보의 예외'에 해당하는 메...  \n",
      "1169  정보성 메시지란 정보통신망법 안내서에 '영리목적 광고성 정보의 예외'에 해당하는 메...  \n",
      "1170  정보성 메시지란 정보통신망법 안내서에 '영리목적 광고성 정보의 예외'에 해당하는 메...  \n",
      "1171  정보성 메시지란 정보통신망법 안내서에 '영리목적 광고성 정보의 예외'에 해당하는 메...  \n",
      "\n",
      "[1172 rows x 3 columns]>\n",
      "\n",
      "Hugging Face Dataset 으로 변환\n",
      "최종 분할된 데이터 셋\n",
      "Dataset({\n",
      "    features: ['template', 'user_input', 'policy'],\n",
      "    num_rows: 937\n",
      "})\n",
      "Dataset({\n",
      "    features: ['template', 'user_input', 'policy'],\n",
      "    num_rows: 235\n",
      "})\n",
      "open 삭제됨\n",
      "AutoTokenizer 삭제됨\n",
      "DataCollatorForSeq2Seq 삭제됨\n",
      "Dataset 삭제됨\n",
      "DatasetDict 삭제됨\n",
      "load_tokenizer_from_local 삭제됨\n",
      "boto3 삭제됨\n",
      "pd 삭제됨\n",
      "io 삭제됨\n",
      "credentials_df 삭제됨\n",
      "aws_access_key_id 삭제됨\n",
      "aws_secret_access_key 삭제됨\n",
      "bucket_name 삭제됨\n",
      "file_key 삭제됨\n",
      "s3_client 삭제됨\n",
      "file_content 삭제됨\n",
      "df 삭제됨\n",
      "gen_dataset 삭제됨\n",
      "gen_train_test_dataset 삭제됨\n"
     ]
    }
   ],
   "source": [
    "# 데이터 셋 불러오기\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "from datasets import Dataset\n",
    "\n",
    "# AWS S3 에 업로드 했던 데이터셋 불러오기\n",
    "# 엑세스키가 필요함\n",
    "try:\n",
    "    credentials_df = pd.read_csv('./ganghyun-dev_accessKeys.csv')\n",
    "\n",
    "    if not credentials_df.empty:\n",
    "        aws_access_key_id = credentials_df['Access key ID'].iloc[0].strip()\n",
    "        aws_secret_access_key = credentials_df['Secret access key'].iloc[0].strip()\n",
    "    else:\n",
    "        print(\"Error: 'aws_credentials.csv' is empty.\")\n",
    "        exit()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'aws_credentials.csv' not found in Drive.\")\n",
    "    print(\"Please create a file named 'aws_credentials.csv' in your Google Drive with your AWS credentials.\")\n",
    "    exit()\n",
    "except KeyError:\n",
    "    print(\"Error: 'Access key ID' or 'Secret access key' column not found in 'aws_credentials.csv'.\")\n",
    "    print(\"Please ensure your CSV file has these columns.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading AWS credentials from CSV: {e}\")\n",
    "    exit()\n",
    "\n",
    "bucket_name = \"dr.hong-s3\"\n",
    "\n",
    "file_key = \"dataset/template_generation_dataset_with_temp_policy.xlsx\"\n",
    "\n",
    "s3_client = boto3.client('s3',\n",
    "                         aws_access_key_id=aws_access_key_id,\n",
    "                         aws_secret_access_key=aws_secret_access_key)\n",
    "\n",
    "try:\n",
    "    file_content = s3_client.get_object(Bucket=bucket_name, Key=file_key)['Body'].read()\n",
    "    print(\"파일을 성공적으로 메모리로 불러왔습니다.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"S3에서 파일을 불러오는 중 오류가 발생했습니다: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 엑셀 파일을 pandas datafrome 으로 변환\n",
    "print(\"파일을 pandas dataframe 로 변환\")\n",
    "df = pd.read_excel(io.BytesIO(file_content))\n",
    "\n",
    "print(\"원본 데이터 상위 5개\")\n",
    "print(df.head)\n",
    "print()\n",
    "\n",
    "print(\"Hugging Face Dataset 으로 변환\")\n",
    "gen_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "gen_train_test_dataset = gen_dataset.train_test_split(test_size=0.2, shuffle=True)\n",
    "gen_train_dataset = gen_train_test_dataset[\"train\"]\n",
    "gen_test_dataset = gen_train_test_dataset[\"test\"]\n",
    "\n",
    "print(\"최종 분할된 데이터 셋\")\n",
    "print(gen_train_dataset)\n",
    "print(gen_test_dataset)\n",
    "\n",
    "cleanup_globals([\"gen_train_dataset\", \"gen_test_dataset\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "execution_state": "idle",
   "id": "ed03d955-3f49-46c1-8431-6353c4f0bef0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T03:49:22.714289Z",
     "iopub.status.busy": "2025-09-05T03:49:22.714059Z",
     "iopub.status.idle": "2025-09-05T03:49:29.437817Z",
     "shell.execute_reply": "2025-09-05T03:49:29.436955Z",
     "shell.execute_reply.started": "2025-09-05T03:49:22.714272Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'MLP-KTLim/llama-3-Korean-Bllossom-8B' 모델을 './downloaded_model/MLP-KTLim--llama-3-Korean-Bllossom-8B' 경로에 다운로드합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 12 files: 100%|██████████| 12/12 [00:00<00:00, 15.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 모델 준비 완료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cleanup_globals' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m checkpoint = \u001b[33m\"\u001b[39m\u001b[33mMLP-KTLim/llama-3-Korean-Bllossom-8B\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     27\u001b[39m model_path = download_model_snapshot(checkpoint, \u001b[33m\"\u001b[39m\u001b[33m./downloaded_model/\u001b[39m\u001b[33m\"\u001b[39m + checkpoint.replace(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m--\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[43mcleanup_globals\u001b[49m([\u001b[33m\"\u001b[39m\u001b[33mgen_train_dataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mgen_test_dataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel_path\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'cleanup_globals' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "from huggingface_hub import snapshot_download\n",
    "from huggingface_hub.utils import RepositoryNotFoundError\n",
    "\n",
    "def download_model_snapshot(model_id: str, local_dir: str) -> str:\n",
    "    print(f\"'{model_id}' 모델을 '{local_dir}' 경로에 다운로드합니다...\")\n",
    "    try:\n",
    "        # snapshot_download는 알아서 기존 파일을 체크하고 필요한 것만 다운로드합니다.\n",
    "        model_path = snapshot_download(\n",
    "            repo_id=model_id,\n",
    "            local_dir=local_dir\n",
    "            # resume_download=True, # 기본값이 True이므로 명시하지 않아도 됨\n",
    "        )\n",
    "        print(\"✅ 모델 준비 완료!\")\n",
    "        return model_path\n",
    "    except RepositoryNotFoundError:\n",
    "        print(f\"❌ 오류: 모델 ID '{model_id}'를 찾을 수 없습니다.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 다운로드 중 오류가 발생했습니다: {e}\")\n",
    "        return None\n",
    "    \n",
    "# 실행\n",
    "checkpoint = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"\n",
    "model_path = download_model_snapshot(checkpoint, \"./downloaded_model/\" + checkpoint.replace(\"/\", \"--\"))\n",
    "\n",
    "cleanup_globals([\"gen_train_dataset\", \"gen_test_dataset\", \"model_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "execution_state": "idle",
   "id": "b78aff30-cdf7-43e5-a6be-7317a30af24c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T03:23:16.213063Z",
     "iopub.status.busy": "2025-09-05T03:23:16.212808Z",
     "iopub.status.idle": "2025-09-05T03:23:19.390229Z",
     "shell.execute_reply": "2025-09-05T03:23:19.389608Z",
     "shell.execute_reply.started": "2025-09-05T03:23:16.213046Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying tokenization function to the dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 937/937 [00:00<00:00, 2138.43 examples/s]\n",
      "Map: 100%|██████████| 235/235 [00:00<00:00, 2042.93 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized train dataset features: {'input_ids': List(Value('int32')), 'attention_mask': List(Value('int8')), 'labels': List(Value('int64'))}\n",
      "\n",
      "Tokenized test dataset features: {'input_ids': List(Value('int32')), 'attention_mask': List(Value('int8')), 'labels': List(Value('int64'))}\n",
      "\n",
      "Train dataset size: 937\n",
      "Test dataset size: 235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 937/937 [00:00<00:00, 4468.79 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 235/235 [00:00<00:00, 1530.62 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen_train_dataset 삭제됨\n",
      "gen_test_dataset 삭제됨\n",
      "AutoTokenizer 삭제됨\n",
      "DataCollatorForSeq2Seq 삭제됨\n",
      "Dataset 삭제됨\n",
      "DatasetDict 삭제됨\n",
      "load_tokenizer_from_local 삭제됨\n",
      "tokenize_function 삭제됨\n",
      "tokenized_gen_train_datasets 삭제됨\n",
      "tokenized_gen_eval_datasets 삭제됨\n",
      "tokenized_gen_datasets 삭제됨\n"
     ]
    }
   ],
   "source": [
    "# 데이터 셋 전처리\n",
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq\n",
    "from datasets import Dataset, DatasetDict\n",
    "import torch\n",
    "\n",
    "# 토크나이저 디스크에서 메모리로 로드\n",
    "def load_tokenizer_from_local(model_path: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = load_tokenizer_from_local(model_path)\n",
    "\n",
    "# 토큰화 함수 재정의\n",
    "def tokenize_function(elements):\n",
    "    # 입력 시퀀스 만들기\n",
    "    texts = [\n",
    "        f\"{tokenizer.bos_token}user_input: {user_input}\\npolicy: {policy}\\ntemplate: {template}{tokenizer.eos_token}\"\n",
    "        for user_input, policy, template in zip(elements['user_input'], elements['policy'], elements['template'])\n",
    "    ]\n",
    "    # 토큰화 하기\n",
    "    tokenized = tokenizer(\n",
    "        texts, \n",
    "        truncation=True, \n",
    "        max_length=8192,\n",
    "        padding=False,  # DataCollator 에서 padding 함\n",
    "    )\n",
    "    \n",
    "    tokenized['labels'] = tokenized['input_ids'].copy() # AutoRegressive\n",
    "    return tokenized\n",
    "\n",
    "# 데이터셋에 토큰화 함수 적용\n",
    "print(\"\\nApplying tokenization function to the dataset...\")\n",
    "tokenized_gen_train_datasets = gen_train_dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True,\n",
    "    remove_columns=gen_train_dataset.column_names\n",
    ")\n",
    "tokenized_gen_eval_datasets = gen_test_dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True,\n",
    "    remove_columns=gen_test_dataset.column_names\n",
    ")\n",
    "\n",
    "tokenized_gen_datasets = DatasetDict({\n",
    "    'train': tokenized_gen_train_datasets,\n",
    "    'eval': tokenized_gen_eval_datasets\n",
    "})\n",
    "\n",
    "print(\"Tokenized train dataset features:\", tokenized_gen_train_datasets.features)\n",
    "print(\"\\nTokenized test dataset features:\", tokenized_gen_eval_datasets.features)\n",
    "print(f\"\\nTrain dataset size: {len(tokenized_gen_train_datasets)}\")\n",
    "print(f\"Test dataset size: {len(tokenized_gen_eval_datasets)}\")\n",
    "\n",
    "tokenized_path = \"./tokenized_datasets/\" + \"MLP-KTLim/llama-3-Korean-Bllossom-8B\".replace(\"/\", \"--\")\n",
    "tokenized_gen_datasets.save_to_disk(tokenized_path)\n",
    "\n",
    "cleanup_globals([\"model_path\", \"tokenizer\", \"tokenized_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "execution_state": "idle",
   "id": "d4cc0790-c7dc-458f-963e-c205617a62b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T05:14:41.880208Z",
     "iopub.status.busy": "2025-09-05T05:14:41.879916Z",
     "iopub.status.idle": "2025-09-05T05:14:42.062878Z",
     "shell.execute_reply": "2025-09-05T05:14:42.062277Z",
     "shell.execute_reply.started": "2025-09-05T05:14:41.880192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 사용 중인 GPU 메모리: 0.00 GB\n",
      "현재 캐시된 GPU 메모리: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# GPU가 사용 가능한지 확인\n",
    "if torch.cuda.is_available():\n",
    "    # 현재 사용 중인 메모리 (바이트)\n",
    "    allocated_bytes = torch.cuda.memory_allocated(device=0)\n",
    "    # 캐시된 메모리 (바이트)\n",
    "    reserved_bytes = torch.cuda.memory_reserved(device=0)\n",
    "\n",
    "    # GB 단위로 변환\n",
    "    gb_factor = 1024 * 1024 * 1024\n",
    "    allocated_gb = allocated_bytes / gb_factor\n",
    "    reserved_gb = reserved_bytes / gb_factor\n",
    "\n",
    "    print(f\"현재 사용 중인 GPU 메모리: {allocated_gb:.2f} GB\")\n",
    "    print(f\"현재 캐시된 GPU 메모리: {reserved_gb:.2f} GB\")\n",
    "\n",
    "else:\n",
    "    print(\"GPU를 사용할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "execution_state": "idle",
   "id": "bb1bc9b2-53a6-4675-ae0a-3985bfe58973",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T05:02:09.912195Z",
     "iopub.status.busy": "2025-09-05T05:02:09.911943Z",
     "iopub.status.idle": "2025-09-05T05:02:10.124355Z",
     "shell.execute_reply": "2025-09-05T05:02:10.123673Z",
     "shell.execute_reply.started": "2025-09-05T05:02:09.912180Z"
    }
   },
   "outputs": [],
   "source": [
    "cleanup_globals([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "execution_state": "idle",
   "id": "3e6106df-097d-4706-80be-e75a79f7dd4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T07:04:27.810776Z",
     "iopub.status.busy": "2025-09-05T07:04:27.810472Z",
     "iopub.status.idle": "2025-09-05T07:07:17.590502Z",
     "shell.execute_reply": "2025-09-05T07:07:17.589799Z",
     "shell.execute_reply.started": "2025-09-05T07:04:27.810756Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb0a6163803d45efad601b4d5e4e5273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA 어댑터 초기화 중...\n",
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n",
      "open 삭제됨\n",
      "AutoTokenizer 삭제됨\n",
      "AutoModelForCausalLM 삭제됨\n",
      "BitsAndBytesConfig 삭제됨\n",
      "LoraConfig 삭제됨\n",
      "get_peft_model 삭제됨\n",
      "prepare_model_for_kbit_training 삭제됨\n",
      "model_path 삭제됨\n",
      "load_model_and_tokenizer_from_local 삭제됨\n",
      "lora_config 삭제됨\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "model_path = \"/mnt/custom-file-systems/s3/shared/downloaded_model/MLP-KTLim--llama-3-Korean-Bllossom-8B\"\n",
    "\n",
    "def load_model_and_tokenizer_from_local(model_path: str):\n",
    "    # load_in_8bit=True 옵션으로 8비트 양자화를 활성화합니다.\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        llm_int8_threshold=6.0, # 양자화에서 제외할 아웃라이어 임계값 설정\n",
    "        llm_int8_has_fp16_weight=False, # 모델의 일부를 FP16으로 유지할지 설정\n",
    "        llm_int8_skip_modules=None, # 8비트 양자화에서 제외할 모듈 목록\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_config,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        dtype=torch.float16\n",
    "    )\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer_from_local(model_path)\n",
    "\n",
    "# 모델을 kbit 학습을 위해 준비\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# LoRA 설정\n",
    "print(\"LoRA 어댑터 초기화 중...\")\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # 학습능력치, 높이면 학습 성능이 올라가지만 메모리와 속도에서 손해\n",
    "    lora_alpha=16, # 가중치 스케일링, 보통 r 의 두배\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # 타겟 모듈 추가\n",
    "    # target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # 더 정밀한 파인튜닝을 하려면 타겟 추가\n",
    "    lora_dropout=0.05, # 과적합이 발생하면 높여볼 수 있음\n",
    "    bias=\"none\", # none 이 일반적임, 다른 옵션은 영향이 거의 없음\n",
    "    task_type=\"CAUSAL_LM\", # 모델에 맞춰서\n",
    "    # inference_mode=False  # 학습 모드 활성화 기본값\n",
    ")\n",
    "\n",
    "# 모델을 PEFT 모델로 변환\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "cleanup_globals([\"tokenizer\", \"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "execution_state": "idle",
   "id": "36bebec0-8eb3-4945-9eb5-bca0b9690a13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T07:07:17.591863Z",
     "iopub.status.busy": "2025-09-05T07:07:17.591488Z",
     "iopub.status.idle": "2025-09-05T07:07:18.923720Z",
     "shell.execute_reply": "2025-09-05T07:07:18.923051Z",
     "shell.execute_reply.started": "2025-09-05T07:07:17.591845Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "tokenized_path = \"./tokenized_datasets/\" + \"MLP-KTLim/llama-3-Korean-Bllossom-8B\".replace(\"/\", \"--\")\n",
    "tokenized_gen_datasets = load_from_disk(tokenized_path)\n",
    "tokenized_gen_train_datasets = tokenized_gen_datasets['train']\n",
    "tokenized_gen_eval_datasets = tokenized_gen_datasets['eval']\n",
    "\n",
    "# DataCollator 정의 - 패딩과 라벨 처리를 위해 DataCollatorForSeq2Seq 사용\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "execution_state": "idle",
   "id": "8aecd728-5c00-4680-8e74-00dc549e7dd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T07:07:18.924736Z",
     "iopub.status.busy": "2025-09-05T07:07:18.924446Z",
     "iopub.status.idle": "2025-09-05T07:46:42.827814Z",
     "shell.execute_reply": "2025-09-05T07:46:42.827073Z",
     "shell.execute_reply.started": "2025-09-05T07:07:18.924720Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbill291104\u001b[0m (\u001b[33mdr-hong\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for wandb.init()..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/custom-file-systems/s3/shared/wandb/run-20250905_070720-tp7c04nc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dr-hong/dr-hong/runs/tp7c04nc' target=\"_blank\">llama-3-Korean-Bllossom-8B-v1</a></strong> to <a href='https://wandb.ai/dr-hong/dr-hong' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dr-hong/dr-hong' target=\"_blank\">https://wandb.ai/dr-hong/dr-hong</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dr-hong/dr-hong/runs/tp7c04nc' target=\"_blank\">https://wandb.ai/dr-hong/dr-hong/runs/tp7c04nc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/transformers/training_args.py:1818: FutureWarning: `torchdynamo` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `torch_compile_backend` instead\n",
      "  warnings.warn(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128001}.\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "Casting fp32 inputs back to torch.bfloat16 for flash-attn compatibility.\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [90/90 37:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.851100</td>\n",
       "      <td>0.712596</td>\n",
       "      <td>0.754690</td>\n",
       "      <td>0.691437</td>\n",
       "      <td>0.754690</td>\n",
       "      <td>0.433456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.686000</td>\n",
       "      <td>0.612044</td>\n",
       "      <td>0.915666</td>\n",
       "      <td>0.883394</td>\n",
       "      <td>0.915666</td>\n",
       "      <td>0.592565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.628600</td>\n",
       "      <td>0.549853</td>\n",
       "      <td>0.888473</td>\n",
       "      <td>0.857504</td>\n",
       "      <td>0.888473</td>\n",
       "      <td>0.677233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.567500</td>\n",
       "      <td>0.512030</td>\n",
       "      <td>0.908745</td>\n",
       "      <td>0.866209</td>\n",
       "      <td>0.908745</td>\n",
       "      <td>0.731451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.482800</td>\n",
       "      <td>0.486619</td>\n",
       "      <td>0.928802</td>\n",
       "      <td>0.874780</td>\n",
       "      <td>0.928802</td>\n",
       "      <td>0.619834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.514900</td>\n",
       "      <td>0.468702</td>\n",
       "      <td>0.898214</td>\n",
       "      <td>0.853021</td>\n",
       "      <td>0.898214</td>\n",
       "      <td>0.629576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.438300</td>\n",
       "      <td>0.457031</td>\n",
       "      <td>0.884503</td>\n",
       "      <td>0.776887</td>\n",
       "      <td>0.884503</td>\n",
       "      <td>0.743210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.477500</td>\n",
       "      <td>0.448802</td>\n",
       "      <td>0.916825</td>\n",
       "      <td>0.872147</td>\n",
       "      <td>0.916825</td>\n",
       "      <td>0.709512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.443300</td>\n",
       "      <td>0.441495</td>\n",
       "      <td>0.942280</td>\n",
       "      <td>0.912324</td>\n",
       "      <td>0.942280</td>\n",
       "      <td>0.764197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.442300</td>\n",
       "      <td>0.436455</td>\n",
       "      <td>0.931395</td>\n",
       "      <td>0.866178</td>\n",
       "      <td>0.931395</td>\n",
       "      <td>0.664749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.423800</td>\n",
       "      <td>0.433619</td>\n",
       "      <td>0.901792</td>\n",
       "      <td>0.871921</td>\n",
       "      <td>0.901792</td>\n",
       "      <td>0.735143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.460900</td>\n",
       "      <td>0.430128</td>\n",
       "      <td>0.876112</td>\n",
       "      <td>0.863254</td>\n",
       "      <td>0.876112</td>\n",
       "      <td>0.738583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.421500</td>\n",
       "      <td>0.430017</td>\n",
       "      <td>0.857937</td>\n",
       "      <td>0.774208</td>\n",
       "      <td>0.835714</td>\n",
       "      <td>0.819977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='59' max='59' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [59/59 00:46]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>▁▄▅▆▄▅▇▆▇▅▆▇█▆</td></tr><tr><td>eval/loss</td><td>█▆▄▃▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>eval/rouge1</td><td>▁▇▆▇▇▆▆▇██▆▆▅▇</td></tr><tr><td>eval/rouge2</td><td>▁▇▆▇▇▆▄▇█▇▇▆▄▇</td></tr><tr><td>eval/rougeL</td><td>▁▇▆▇▇▆▆▇██▆▆▄▇</td></tr><tr><td>eval/runtime</td><td>▂▁▁▁▁▁▁▁▂▁▁▁▁█</td></tr><tr><td>eval/samples_per_second</td><td>▇███████▇████▁</td></tr><tr><td>eval/steps_per_second</td><td>▇███████▇████▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇██████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇██████</td></tr><tr><td>+3</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>0.71261</td></tr><tr><td>eval/loss</td><td>0.42933</td></tr><tr><td>eval/rouge1</td><td>0.90319</td></tr><tr><td>eval/rouge2</td><td>0.89441</td></tr><tr><td>eval/rougeL</td><td>0.90319</td></tr><tr><td>eval/runtime</td><td>47.9964</td></tr><tr><td>eval/samples_per_second</td><td>4.896</td></tr><tr><td>eval/steps_per_second</td><td>1.229</td></tr><tr><td>total_flos</td><td>5.449679907087974e+16</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>+8</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">llama-3-Korean-Bllossom-8B-v1</strong> at: <a href='https://wandb.ai/dr-hong/dr-hong/runs/tp7c04nc' target=\"_blank\">https://wandb.ai/dr-hong/dr-hong/runs/tp7c04nc</a><br> View project at: <a href='https://wandb.ai/dr-hong/dr-hong' target=\"_blank\">https://wandb.ai/dr-hong/dr-hong</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250905_070720-tp7c04nc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import wandb\n",
    "import evaluate\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# wandb 초기화\n",
    "wandb.init( # 기록을 위한 정보이며 학습에 영향을 주지 않음\n",
    "    entity=\"dr-hong\",\n",
    "    project=\"dr-hong\",\n",
    "    name=\"llama-3-Korean-Bllossom-8B-v1\",\n",
    "    config={\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"epochs\": 3,\n",
    "        \"batch_size\": 4,\n",
    "        \"model_name\": \"MLP-KTLim/llama-3-Korean-Bllossom-8B\",\n",
    "        \"lora_r\": 8,\n",
    "        \"lora_alpha\": 16,\n",
    "    }\n",
    ")\n",
    "\n",
    "# compute_metrics 함수 정의. llama3 에 적합한 평가지표 선택: ROUGE, BLEU\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "def compute_metrics(eval_preds, compute_result=False):\n",
    "    predictions, labels = eval_preds.predictions, eval_preds.label_ids\n",
    "    predictions = predictions.argmax(axis=-1)  # logits → token ids\n",
    "\n",
    "    labels[labels == -100] = tokenizer.pad_token_id\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    rouge_result = rouge.compute(\n",
    "        predictions=decoded_preds, \n",
    "        references=decoded_labels\n",
    "    )\n",
    "    \n",
    "    bleu_result = bleu.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=[[label] for label in decoded_labels]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"rouge1\": rouge_result[\"rouge1\"],\n",
    "        \"rouge2\": rouge_result[\"rouge2\"],\n",
    "        \"rougeL\": rouge_result[\"rougeL\"],\n",
    "        \"bleu\": bleu_result[\"bleu\"]\n",
    "    }\n",
    "\n",
    "# 학습 인자 정의\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/\" + \"MLP-KTLim/llama-3-Korean-Bllossom-8B\".replace(\"/\", \"--\"),\n",
    "    # overwrite_output_dir=True, # False 기본값이면 자동으로 중단되었던 학습을 이어서 할 수 있음, True 면 처음부터 다시\n",
    "    do_train=True, # 학습 단계를 건너뛸 수도 있음\n",
    "    do_eval=True, # 평가를 따로 진행할 수 있음\n",
    "    # do_predict=False, # 추론단계는 기본적으로 안함\n",
    "    eval_strategy=\"steps\", # 일정 step 마다 평가\n",
    "    # prediction_loss_only=False, # True 면 평가와 추론 작업에서 loss 만 반환함\n",
    "    per_device_train_batch_size=4, # 학습 배치 사이즈, 기본은 8인데 메모리 절약을 위해 줄임\n",
    "    per_device_eval_batch_size=4, # 평가 배치 사이즈, 기본은 8인데 메모리 절약을 위해 줄임\n",
    "    gradient_accumulation_steps=8, # 줄인 배치사이즈를 보완하는 배수, 실제 효과는 배치사이즈와 이 값을 곱한 크기를 배치사이즈로 하는 것과 동일함\n",
    "    eval_accumulation_steps=64, # 평가 단계에서 GPU 메모리 문제가 발생하면 설정, 키우면 CPU 메모리로 더 자주 결과를 옮김\n",
    "    eval_delay=30, # 학습 초기 단계에서 평가를 건너뜀, 의미없는 계산을 줄임. 학습셋 937개 / (실질적 배치 사이즈 4 * 8) ~= 30 -> 1epoch 정도는 평가를 건너뜀\n",
    "    torch_empty_cache_steps=30, # GPU 메모리 정리 함수를 호출하는 시점. 안전하게 eval_step 과 동일한 값으로\n",
    "    learning_rate=2e-4, # AdamW 옵티마이저를 위한 초기 학습률. LoRA 를 쓴다면 2e-4 가 좋음\n",
    "    weight_decay=0.01, # 정규화 기능. 일반적으로 0.01 사용. 과적합이면 높임\n",
    "    # adam_beta1, adam_beta2, adam_epsilon, max_grad_norm 은 기본값으로 사용\n",
    "    # num_train_epochs=3, # 학습 횟수. 기본 3\n",
    "    # max_steps 사용 안함\n",
    "    # lr_scheduler_type, lr_scheduler_kwargs 는 기본값 사용\n",
    "    warmup_ratio=0.1, # 10% 를 웜업 단계로 설정\n",
    "    # warmup_step=9 # 3 에포크 * 30 스텝 * 0.1 = 9\n",
    "    # log_level=\"passive\", # 로그 레벨 기본값 사용. warning\n",
    "    # log_level_replica, log_on_each_node 는 분산 학습용\n",
    "    logging_dir=\"./logs/\" + \"MLP-KTLim/llama-3-Korean-Bllossom-8B\".replace(\"/\", \"--\"), # TensorBoard 로 로그 확인 가능\n",
    "    logging_strategy=\"steps\", # epoch 가 얼마 없어서 steps 사용\n",
    "    # logging_first_step=False\n",
    "    logging_steps=5, # 1 epoch 와 동일하게\n",
    "    # logging_nan_inf_filter=True # 기본 True. nan, inf 를 무시하고 로그를 깔끔하게\n",
    "    save_strategy=\"steps\", # logging_strategy 와 동일하게\n",
    "    save_steps=15, # epoch 전략이면 무시됨\n",
    "    save_total_limit=1, # 저장되는 체크포인트 한개만 유지\n",
    "    # save_safetensors=True # 기본 True. sagetensor 형식으로 저장\n",
    "    # save_on_each_node 는 분산 학습에서 사용\n",
    "    # save_only_model=False # 이어서 학습이 가능하도록 기본값 False 로 유지\n",
    "    # restore_callback_states_from_checkpoint 은 콜백 기능을 사용하면 설정 가능\n",
    "    # use_cpu=False # GPU 사용하므로 기본값 False 유지\n",
    "    # seed, data_seed 는 기본값 사용\n",
    "    # use_ipex=False # NVIDIA GPU 를 사용하므로 기본값 False 사용\n",
    "    bf16=True, # 최신 GPU 라면 bf16, 구형이라면 fp16 사용\n",
    "    # half_precision_backend=\"auto\" # 혼합 정밀도 기능을 위한 라이브러리, 기본값 auto 사용\n",
    "    tf32=True, # ml.g5.xlarge 인스턴스에서 작업하므로 사용가능한 옵션\n",
    "    # local_rank=-1 # 분산 학습이 아니므로 기본값 -1\n",
    "    # ddp_backend=\"nccl\" # 분산 학습이면 사용\n",
    "    eval_steps=5, # eval_strategy 가 steps 면 logging_steps 와 동일한 값이 기본\n",
    "    dataloader_num_workers=4, # ml.g5.xlarge 는 cpu 코어수가 4개. 데이터로더가 병렬처리 가능\n",
    "    disable_tqdm=False, # 진행도 시각화 기능 사용\n",
    "    # remove_unused_columns=True, # 메모리 절약을 위해 기본값 True 사용\n",
    "    # label_names 는 label 을 사용하는 것이 기본이고 그렇게 전처리 했음\n",
    "    load_best_model_at_end=True, # save_total_limit=1 과 조합이 좋음\n",
    "    metric_for_best_model=\"eval_loss\", # 가장 좋은 모델 선택 기준: eval loss\n",
    "    greater_is_better=False, # loss 는 낮은게 좋은 결과임\n",
    "    # ignore_data_skip=False, # 이어서 학습 하기위해 기본값 False 사용\n",
    "    # fsdp, fsdp_config, accelerator_config, parallelism_config 는 분산 학습에서 사용\n",
    "    # label_smoothing_factor=0.1 # 과적합이 발생하면 설정\n",
    "    # optim, optim_args 는 기본인 AdamW 를 사용\n",
    "    group_by_length=True, # Dynamic Padding 을 했다면 메모리 절약 가능\n",
    "    report_to=\"all\", # 모든 사용 가능한 플랫폼에 보고, 지금은 wandb\n",
    "    # ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers 는 분산 환경에서 사용\n",
    "    # dataloader_pin_memory=True, # GPU -> CPU 메모리 이동이 빨라짐, 기본값 True 사용\n",
    "    dataloader_persistent_workers=True, # CPU RAM 이 부족하면 False 로\n",
    "    # dataloader_prefetch_factor=2, # CPU 메모리가 여유롭다면 2 이상으로 사용 가능\n",
    "    # skip_memory_metrics=True # 메모리 문제의 원인을 찾아야 한다면 False 사용\n",
    "    # push_to_hub=False # 업로드 안함\n",
    "    resume_from_checkpoint=\"./results/\" + \"MLP-KTLim/llama-3-Korean-Bllossom-8B\".replace(\"/\", \"--\"), # 학습 재개 가능\n",
    "    gradient_checkpointing=True, # 메모리 문제 방지\n",
    "    # eval_do_concat_batches=True, # 평가지표 계산 속도를 위해 기본값 True 사용\n",
    "    auto_find_batch_size=True, # 다른 배치 사이즈 설정을 덮어씀. 자동으로 실행가능한 최적의 배치 크기를 찾아줌. 처음 한번은 이 기능을 사용해서 배치 크기를 찾는게 좋음\n",
    "    # full_determinism=False, # 난수 고정 기능. 연구에 사용 가능\n",
    "    torchdynamo=\"inductor\", # 학습 속도 올려주는 PyTorch 기본 라이브러리\n",
    "    torch_compile=True, # 모델 최적화 기능\n",
    "    torch_compile_backend=\"inductor\", # 컴파일 해주는 백엔드 지정\n",
    "    torch_compile_mode=\"default\",\n",
    "    neftune_noise_alpha=5.0, # llama 3 같은 명령어 파인튜닝에 효과적인 기법\n",
    "    batch_eval_metrics=True, # 평가단계 메모리 절약\n",
    "    # eval_on_start=True, # 평가단계 디버깅을 빠르게 해볼려면 사용\n",
    ")\n",
    "\n",
    "# Trainer 설정\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_gen_train_datasets,\n",
    "    eval_dataset=tokenized_gen_eval_datasets,\n",
    "    processing_class=tokenizer,\n",
    "    # model_init 는 하이퍼 파라미터 튜닝할 때 사용\n",
    "    compute_metrics=compute_metrics,\n",
    "    # optimizers 는 기본 AdamW 사용\n",
    ")\n",
    "\n",
    "try:\n",
    "    train_output = trainer.train()\n",
    "    \n",
    "    eval_results = trainer.evaluate()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"학습 중 오류 발생: {str(e)}\")\n",
    "    raise e\n",
    "\n",
    "# 최종 모델 저장\n",
    "model.save_pretrained(\"./finetuned_model/\" + \"MLP-KTLim/llama-3-Korean-Bllossom-8B\".replace(\"/\", \"--\"))\n",
    "\n",
    "# wandb 종료\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "execution_state": "idle",
   "id": "7818c3c3-222c-4e23-a3b1-9825d77df17b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T08:26:33.551647Z",
     "iopub.status.busy": "2025-09-05T08:26:33.551248Z",
     "iopub.status.idle": "2025-09-05T08:29:10.325298Z",
     "shell.execute_reply": "2025-09-05T08:29:10.324641Z",
     "shell.execute_reply.started": "2025-09-05T08:26:33.551630Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8a983d04344b9192d621da2eb806cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"title\": \"회사소개서 발송\", \"text\": \"안녕하세요 #{수신자명}님,\\n\\n#{회사명}은 #{업종} 분야에서 활동하는 #{회사명}입니다.\\n\\n▶ 회사명 : #{회사명}\\n▶ 업종 : #{업종}\\n▶ 연락처 : #{연락처}\\n\\n감사합니다.\", \"button_name\": \"자세히 보기\"}목적: 고객에게 정보를 제공하는 것이 목적이며, 알림톡으로 발송 가능한 정보성 메시지입니다. 정보통신망법과 카카오톡 내부 기준에 따라 심사 진행되고 승인된 알림톡 템\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# 메모리 정리\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# 베이스 모델과 토크나이저 로드\n",
    "base_model_path = \"/mnt/custom-file-systems/s3/shared/downloaded_model/MLP-KTLim--llama-3-Korean-Bllossom-8B\"\n",
    "lora_adapter_path = \"./finetuned_model/\" + \"MLP-KTLim/llama-3-Korean-Bllossom-8B\".replace(\"/\", \"--\")\n",
    "\n",
    "# 8비트 양자화 설정\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0\n",
    ")\n",
    "\n",
    "# 추론용 모델 로드 (메모리 효율적 버전)\n",
    "def load_inference_model_and_tokenizer(base_model_path, lora_adapter_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_path,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(base_model, lora_adapter_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "base_model, tokenizer = load_inference_model_and_tokenizer(base_model_path, lora_adapter_path)\n",
    "\n",
    "# 추론 함수\n",
    "def generate_template(user_input, policy):\n",
    "    prompt = f\"\"\"사용자 요청: {user_input}\n",
    "정책: {policy}\n",
    "템플릿:\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True).to(base_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = base_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            temperature=0.3,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response.split(\"템플릿:\")[-1].strip()\n",
    "\n",
    "# 사용 예시\n",
    "user_request = \"회사소개서 발송 템플릿 제작 부탁드려요\"\n",
    "policy_text = \"정보성 메시지란 정보통신망법 안내서에 '영리목적 광고성 정보의 예외'에 해당하는 메시지입니다.\"\n",
    "\n",
    "result = generate_template(user_request, policy_text)\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dr-hong-pr",
   "language": "python",
   "name": "dr-hong-pr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
