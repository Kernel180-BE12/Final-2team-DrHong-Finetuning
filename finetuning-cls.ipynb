{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5a46bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "def cleanup_globals(vars_to_keep: list):\n",
    "    global_vars = list(globals().keys())\n",
    "    protected_vars = ['In', 'Out', 'get_ipython', 'exit', 'quit', 'gc', 'torch', 'cleanup_globals']\n",
    "\n",
    "    for var in global_vars:\n",
    "        if var not in vars_to_keep and not var.startswith('_') and var not in protected_vars:\n",
    "            try:\n",
    "                del globals()[var]\n",
    "                print(f\"{var} 삭제됨\")\n",
    "            except:\n",
    "                continue\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8de442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "from datasets import Dataset\n",
    "\n",
    "try:\n",
    "    credentials_df = pd.read_csv('./ganghyun-dev_accessKeys.csv')\n",
    "\n",
    "    if not credentials_df.empty:\n",
    "        aws_access_key_id = credentials_df['Access key ID'].iloc[0].strip()\n",
    "        aws_secret_access_key = credentials_df['Secret access key'].iloc[0].strip()\n",
    "    else:\n",
    "        print(\"Error: 'aws_credentials.csv' is empty.\")\n",
    "        exit()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'aws_credentials.csv' not found in Drive.\")\n",
    "    print(\"Please create a file named 'aws_credentials.csv' in your Google Drive with your AWS credentials.\")\n",
    "    exit()\n",
    "except KeyError:\n",
    "    print(\"Error: 'Access key ID' or 'Secret access key' column not found in 'aws_credentials.csv'.\")\n",
    "    print(\"Please ensure your CSV file has these columns.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading AWS credentials from CSV: {e}\")\n",
    "    exit()\n",
    "\n",
    "bucket_name = \"dr.hong-s3\"\n",
    "\n",
    "file_key = \"dataset/origin_template_classification_dataset.xlsx\"\n",
    "\n",
    "s3_client = boto3.client('s3',\n",
    "                         aws_access_key_id=aws_access_key_id,\n",
    "                         aws_secret_access_key=aws_secret_access_key)\n",
    "\n",
    "try:\n",
    "    file_content = s3_client.get_object(Bucket=bucket_name, Key=file_key)['Body'].read()\n",
    "    print(\"파일을 성공적으로 메모리로 불러왔습니다.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"S3에서 파일을 불러오는 중 오류가 발생했습니다: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 엑셀 파일을 pandas datafrome 으로 변환\n",
    "print(\"파일을 pandas dataframe 로 변환\")\n",
    "df = pd.read_excel(io.BytesIO(file_content)).fillna(None)\n",
    "\n",
    "print(\"원본 데이터 상위 5개\")\n",
    "print(df.head)\n",
    "print()\n",
    "\n",
    "print(\"Hugging Face Dataset 으로 변환\")\n",
    "cls_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "cls_train_test_dataset = cls_dataset.train_test_split(test_size=0.2, shuffle=True)\n",
    "cls_train_dataset = cls_train_test_dataset[\"train\"]\n",
    "cls_test_dataset = cls_train_test_dataset[\"test\"]\n",
    "\n",
    "print(\"최종 분할된 데이터 셋\")\n",
    "print(cls_train_dataset)\n",
    "print(cls_test_dataset)\n",
    "\n",
    "cleanup_globals([\"cls_train_dataset\", \"cls_test_dataset\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca848df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "from huggingface_hub import snapshot_download\n",
    "from huggingface_hub.utils import RepositoryNotFoundError\n",
    "\n",
    "def download_model_snapshot(model_id: str, local_dir: str) -> str:\n",
    "    print(f\"'{model_id}' 모델을 '{local_dir}' 경로에 다운로드합니다...\")\n",
    "    try:\n",
    "        # snapshot_download는 알아서 기존 파일을 체크하고 필요한 것만 다운로드합니다.\n",
    "        model_path = snapshot_download(\n",
    "            repo_id=model_id,\n",
    "            local_dir=local_dir\n",
    "            # resume_download=True, # 기본값이 True이므로 명시하지 않아도 됨\n",
    "        )\n",
    "        print(\"✅ 모델 준비 완료!\")\n",
    "        return model_path\n",
    "    except RepositoryNotFoundError:\n",
    "        print(f\"❌ 오류: 모델 ID '{model_id}'를 찾을 수 없습니다.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 다운로드 중 오류가 발생했습니다: {e}\")\n",
    "        return None\n",
    "\n",
    "# 실행\n",
    "checkpoint = \"klue/bert-base\" # CC-BY-SA-4.0 라이선스 제약 있음\n",
    "model_path = download_model_snapshot(checkpoint, \"./downloaded_model/\" + checkpoint.replace(\"/\", \"--\"))\n",
    "\n",
    "cleanup_globals([\"cls_train_dataset\", \"cls_test_dataset\", \"model_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5156a67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 셋 전처리\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import DatasetDict\n",
    "import torch\n",
    "\n",
    "# 토크나이저 디스크에서 메모리로 로드\n",
    "def load_tokenizer_from_local(model_path: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = load_tokenizer_from_local(model_path)\n",
    "\n",
    "def tokenize_function(elements):\n",
    "    tokenized = tokenizer(\n",
    "        text=elements['template'],\n",
    "        text_pair=[reason if reason is not None else None for reason in elements['reason']],\n",
    "        padding=False, # DataCollator 에서 padding 함\n",
    "        max_length=512,\n",
    "    )\n",
    "    tokenized[\"labels\"] = elements[\"is_approved\"]\n",
    "    return tokenized\n",
    "\n",
    "# 데이터셋에 토큰화 함수 적용\n",
    "print(\"\\nApplying tokenization function to the dataset...\")\n",
    "tokenized_cls_train_datasets = cls_train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=cls_train_dataset.column_names\n",
    ")\n",
    "tokenized_cls_eval_datasets = cls_test_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=cls_test_dataset.column_names\n",
    ")\n",
    "\n",
    "tokenized_cls_datasets = DatasetDict({\n",
    "    \"train\": tokenized_cls_train_datasets,\n",
    "    \"eval\": tokenized_cls_eval_datasets\n",
    "})\n",
    "\n",
    "print(\"Tokenized train dataset features:\", tokenized_cls_train_datasets.features)\n",
    "print(\"\\nTokenized test dataset features:\", tokenized_cls_eval_datasets.features)\n",
    "print(f\"\\nTrain dataset size: {len(tokenized_cls_train_datasets)}\")\n",
    "print(f\"Test dataset size: {len(tokenized_cls_eval_datasets)}\")\n",
    "\n",
    "tokenized_path = \"./tokenized_datasets/\" + \"klue/bert-base\".replace(\"/\", \"--\")\n",
    "tokenized_gen_datasets.save_to_disk(tokenized_path)\n",
    "\n",
    "cleanup_globals([\"model_path\", \"tokenizer\", \"tokenized_path\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d42036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# GPU가 사용 가능한지 확인\n",
    "if torch.cuda.is_available():\n",
    "    # 현재 사용 중인 메모리 (바이트)\n",
    "    allocated_bytes = torch.cuda.memory_allocated(device=0)\n",
    "    # 캐시된 메모리 (바이트)\n",
    "    reserved_bytes = torch.cuda.memory_reserved(device=0)\n",
    "\n",
    "    # GB 단위로 변환\n",
    "    gb_factor = 1024 * 1024 * 1024\n",
    "    allocated_gb = allocated_bytes / gb_factor\n",
    "    reserved_gb = reserved_bytes / gb_factor\n",
    "\n",
    "    print(f\"현재 사용 중인 GPU 메모리: {allocated_gb:.2f} GB\")\n",
    "    print(f\"현재 캐시된 GPU 메모리: {reserved_gb:.2f} GB\")\n",
    "\n",
    "else:\n",
    "    print(\"GPU를 사용할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb2b49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_globals([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c47227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transtormers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_path = \"./downloaded_model/klue--bert-base\"\n",
    "\n",
    "def load_model_and_tokenizer_from_local(model_path: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_path,\n",
    "        local_files_only=True,\n",
    "        # classifier_dropout=0.2, # 과적합 발생하면 시도\n",
    "        num_labels=2,\n",
    "        id2label={0: \"Not Approved\", 1: \"Approved\"},\n",
    "        label2id={\"Not Approved\": 0, \"Approved\": 1},\n",
    "        problem_type=\"single_label_classification\"\n",
    "    )\n",
    "    return tokenizer, model\n",
    "\n",
    "tokenizer, model = load_model_and_tokenizer_from_local(model_path)\n",
    "\n",
    "cleanup_globals([\"tokenizer\", \"model\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c290e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "tokenized_path = \"./tokenized_datasets/\" + \"klue/bert-base\".replace(\"/\", \"--\")\n",
    "tokenized_cls_datasets = load_from_disk(tokenized_path)\n",
    "tokenized_cls_train_datasets = tokenized_cls_datasets['train']\n",
    "tokenized_cls_eval_datasets = tokenized_cls_datasets['eval']\n",
    "\n",
    "# DataCollator 정의 - 텍스트 분류는 DataCollatorWithPadding 사용\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,\n",
    "    padding='longest', # 배치 내에서 가장 긴 시퀀스에 맞춰 패딩\n",
    "    pad_to_multiple_of=8, # 학습 속도를 약간 높여줌\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5972f931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# wandb 초기화\n",
    "wandb.init(\n",
    "    entity=\"dr-hong\",\n",
    "    project=\"dr-hong\",\n",
    "    name=\"klue-bert-base\",\n",
    "    config={\n",
    "        \"learning_rate\": 3e-5,\n",
    "        \"epochs\": 3,\n",
    "        \"batch_size\": 32,\n",
    "        \"model_name\": \"klue/bert-base\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# compute_metrics 함수 정의. bert 에 적합한 평가지표 선택: accuracy, f1, precision, recall\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = predictions.argmax(axis=1)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# 학습 인자 정의\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/\" + \"klue/bert-base\".replace(\"/\", \"--\"),\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=1,\n",
    "    eval_delay=32,\n",
    "    learning_rate=3e-5, # differential learning rate 적용 고려해 볼 수 있음\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_dir=\"./logs/\" + \"klue/bert-base\".replace(\"/\", \"--\"),\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=15,\n",
    "    save_total_limit=1,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    eval_steps=8,\n",
    "    dataloader_num_workers=4,\n",
    "    disable_tqdm=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"all\",\n",
    "    dataloader_persistent_workers=True,\n",
    "    resume_from_checkpoint=\"./results/\" + \"klue/bert-base\".replace(\"/\", \"--\"),\n",
    "    gradient_checkpointing=True,\n",
    "    auto_find_batch_size=True,\n",
    "    torchdynamo=\"inductor\",\n",
    "    torch_compile=True,\n",
    "    torch_compile_backend=\"inductor\",\n",
    "    torch_compile_mode=\"default\",\n",
    "    batch_eval_metrics=True,\n",
    ")\n",
    "\n",
    "# Trainer 설정\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_cls_train_datasets,\n",
    "    eval_dataset=tokenized_cls_eval_datasets,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "try:\n",
    "    train_output = trainer.train()\n",
    "\n",
    "    eval_results = trainer.evaluate()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"학습 중 오류 발생: {str(e)}\")\n",
    "    raise e\n",
    "\n",
    "# 최종 모델 저장\n",
    "model.save_pretrained(\"./final_model/\" + \"klue/bert-base\".replace(\"/\", \"--\"))\n",
    "\n",
    "# wandb 종료\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
