{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "execution_state": "idle",
   "id": "0d5a46bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T01:18:29.146303Z",
     "iopub.status.busy": "2025-09-10T01:18:29.146059Z",
     "iopub.status.idle": "2025-09-10T01:18:34.264971Z",
     "shell.execute_reply": "2025-09-10T01:18:34.264308Z",
     "shell.execute_reply.started": "2025-09-10T01:18:29.146283Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "def cleanup_globals(vars_to_keep: list):\n",
    "    global_vars = list(globals().keys())\n",
    "    protected_vars = ['In', 'Out', 'get_ipython', 'exit', 'quit', 'gc', 'torch', 'cleanup_globals']\n",
    "\n",
    "    for var in global_vars:\n",
    "        if var not in vars_to_keep and not var.startswith('_') and var not in protected_vars:\n",
    "            try:\n",
    "                del globals()[var]\n",
    "                print(f\"{var} 삭제됨\")\n",
    "            except:\n",
    "                continue\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "execution_state": "idle",
   "id": "cc8de442",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T01:19:39.307896Z",
     "iopub.status.busy": "2025-09-10T01:19:39.307643Z",
     "iopub.status.idle": "2025-09-10T01:19:40.872448Z",
     "shell.execute_reply": "2025-09-10T01:19:40.871740Z",
     "shell.execute_reply.started": "2025-09-10T01:19:39.307879Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일을 성공적으로 메모리로 불러왔습니다.\n",
      "파일을 pandas dataframe 로 변환\n",
      "원본 데이터 상위 5개\n",
      "<bound method NDFrame.head of                                                template  is_approved  \\\n",
      "0     {\"title\": \"회사소개서 발송\", \"text\": \"안녕하세요 #{수신자명}님,...            1   \n",
      "1     {\"title\": \"서비스 소개서 발송\", \"text\": \"안녕하세요 #{수신자명}...            1   \n",
      "2     {\"title\": \"(전용) 강의 일정 안내 / 화케터\", \"text\": \"안녕하세...            1   \n",
      "3     {\"title\": \"(공용) 후기 작성 요청_이미지형_01\", \"text\": \"[템...            1   \n",
      "4     {\"title\": \"(공용) 인보이스 알림_이미지형_01\", \"text\": \"■ #...            1   \n",
      "...                                                 ...          ...   \n",
      "1261  {\"title\": \"(공용) 적립금 소멸 안내_이미지형_01\", \"text\": \"안...            0   \n",
      "1262  {\"title\": \"(전용) AS 안내_기본형_01 / 라벨르\", \"text\": \"...            0   \n",
      "1263  {\"title\": \"(공용) 문서 도착 알림_이미지형_08\", \"text\": \"■ ...            0   \n",
      "1264  {\"title\": \"(공용) 링크_ 문서 도착 알림_이미지형_01\", \"text\":...            0   \n",
      "1265  {\"title\": \"(공용) 링크_문서 도착 알림_이미지형_01\", \"text\": ...            0   \n",
      "\n",
      "     reject_reason  \n",
      "0              NaN  \n",
      "1              NaN  \n",
      "2              NaN  \n",
      "3              NaN  \n",
      "4              NaN  \n",
      "...            ...  \n",
      "1261           NaN  \n",
      "1262           NaN  \n",
      "1263           NaN  \n",
      "1264           NaN  \n",
      "1265           NaN  \n",
      "\n",
      "[1266 rows x 3 columns]>\n",
      "\n",
      "Hugging Face Dataset 으로 변환\n",
      "최종 분할된 데이터 셋\n",
      "Dataset({\n",
      "    features: ['template', 'is_approved', 'reject_reason'],\n",
      "    num_rows: 1012\n",
      "})\n",
      "Dataset({\n",
      "    features: ['template', 'is_approved', 'reject_reason'],\n",
      "    num_rows: 254\n",
      "})\n",
      "open 삭제됨\n",
      "boto3 삭제됨\n",
      "pd 삭제됨\n",
      "io 삭제됨\n",
      "Dataset 삭제됨\n",
      "credentials_df 삭제됨\n",
      "aws_access_key_id 삭제됨\n",
      "aws_secret_access_key 삭제됨\n",
      "bucket_name 삭제됨\n",
      "file_key 삭제됨\n",
      "s3_client 삭제됨\n",
      "file_content 삭제됨\n",
      "df 삭제됨\n",
      "cls_dataset 삭제됨\n",
      "cls_train_test_dataset 삭제됨\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "from datasets import Dataset\n",
    "\n",
    "try:\n",
    "    credentials_df = pd.read_csv('./ganghyun-dev_accessKeys.csv')\n",
    "\n",
    "    if not credentials_df.empty:\n",
    "        aws_access_key_id = credentials_df['Access key ID'].iloc[0].strip()\n",
    "        aws_secret_access_key = credentials_df['Secret access key'].iloc[0].strip()\n",
    "    else:\n",
    "        print(\"Error: 'aws_credentials.csv' is empty.\")\n",
    "        exit()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'aws_credentials.csv' not found in Drive.\")\n",
    "    print(\"Please create a file named 'aws_credentials.csv' in your Google Drive with your AWS credentials.\")\n",
    "    exit()\n",
    "except KeyError:\n",
    "    print(\"Error: 'Access key ID' or 'Secret access key' column not found in 'aws_credentials.csv'.\")\n",
    "    print(\"Please ensure your CSV file has these columns.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading AWS credentials from CSV: {e}\")\n",
    "    exit()\n",
    "\n",
    "bucket_name = \"dr.hong-s3\"\n",
    "\n",
    "file_key = \"dataset/origin_template_classification_dataset.xlsx\"\n",
    "\n",
    "s3_client = boto3.client('s3',\n",
    "                         aws_access_key_id=aws_access_key_id,\n",
    "                         aws_secret_access_key=aws_secret_access_key)\n",
    "\n",
    "try:\n",
    "    file_content = s3_client.get_object(Bucket=bucket_name, Key=file_key)['Body'].read()\n",
    "    print(\"파일을 성공적으로 메모리로 불러왔습니다.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"S3에서 파일을 불러오는 중 오류가 발생했습니다: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 엑셀 파일을 pandas datafrome 으로 변환\n",
    "print(\"파일을 pandas dataframe 로 변환\")\n",
    "df = pd.read_excel(io.BytesIO(file_content))\n",
    "\n",
    "print(\"원본 데이터 상위 5개\")\n",
    "print(df.head)\n",
    "print()\n",
    "\n",
    "print(\"Hugging Face Dataset 으로 변환\")\n",
    "cls_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "cls_train_test_dataset = cls_dataset.train_test_split(test_size=0.2, shuffle=True)\n",
    "cls_train_dataset = cls_train_test_dataset[\"train\"]\n",
    "cls_test_dataset = cls_train_test_dataset[\"test\"]\n",
    "\n",
    "print(\"최종 분할된 데이터 셋\")\n",
    "print(cls_train_dataset)\n",
    "print(cls_test_dataset)\n",
    "\n",
    "cleanup_globals([\"cls_train_dataset\", \"cls_test_dataset\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "execution_state": "idle",
   "id": "5ca848df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T01:19:55.835642Z",
     "iopub.status.busy": "2025-09-10T01:19:55.835412Z",
     "iopub.status.idle": "2025-09-10T01:21:09.014753Z",
     "shell.execute_reply": "2025-09-10T01:21:09.014115Z",
     "shell.execute_reply.started": "2025-09-10T01:19:55.835625Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'klue/bert-base' 모델을 './downloaded_model/klue--bert-base' 경로에 다운로드합니다...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f5e7fd66b0d4732a81f362237f72550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b8f9692fa164b2fadc39ab80aaca06f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/425 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c67c86255fc447db42478a9224af723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b4c3526249e4a579e5e722ced09bfab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1106fe5927347249db6f3128ad7eb89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/289 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a66213d6a24a459e809ab2e8dfd8f488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a0bcee7e18451db5bc8224416412d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/744 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d2b2c3c4eb54d5fba86b162f0f66251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ab46ac53864b87baae69aaf03820cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a898597cd6294c2d85e7fc4420e77d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 모델 준비 완료!\n",
      "os 삭제됨\n",
      "snapshot_download 삭제됨\n",
      "RepositoryNotFoundError 삭제됨\n",
      "download_model_snapshot 삭제됨\n",
      "checkpoint 삭제됨\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "from huggingface_hub import snapshot_download\n",
    "from huggingface_hub.utils import RepositoryNotFoundError\n",
    "\n",
    "def download_model_snapshot(model_id: str, local_dir: str) -> str:\n",
    "    print(f\"'{model_id}' 모델을 '{local_dir}' 경로에 다운로드합니다...\")\n",
    "    try:\n",
    "        # snapshot_download는 알아서 기존 파일을 체크하고 필요한 것만 다운로드합니다.\n",
    "        model_path = snapshot_download(\n",
    "            repo_id=model_id,\n",
    "            local_dir=local_dir\n",
    "            # resume_download=True, # 기본값이 True이므로 명시하지 않아도 됨\n",
    "        )\n",
    "        print(\"✅ 모델 준비 완료!\")\n",
    "        return model_path\n",
    "    except RepositoryNotFoundError:\n",
    "        print(f\"❌ 오류: 모델 ID '{model_id}'를 찾을 수 없습니다.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 다운로드 중 오류가 발생했습니다: {e}\")\n",
    "        return None\n",
    "\n",
    "# 실행\n",
    "checkpoint = \"klue/bert-base\" # CC-BY-SA-4.0 라이선스 제약 있음\n",
    "model_path = download_model_snapshot(checkpoint, \"./downloaded_model/\" + checkpoint.replace(\"/\", \"--\"))\n",
    "\n",
    "cleanup_globals([\"cls_train_dataset\", \"cls_test_dataset\", \"model_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "execution_state": "idle",
   "id": "5156a67d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T01:35:44.142837Z",
     "iopub.status.busy": "2025-09-10T01:35:44.142582Z",
     "iopub.status.idle": "2025-09-10T01:35:46.163985Z",
     "shell.execute_reply": "2025-09-10T01:35:46.163423Z",
     "shell.execute_reply.started": "2025-09-10T01:35:44.142821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying tokenization function to the dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb352b663abf49eea711caea972847e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1012 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c305ca1194b45c29030822744c03335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/254 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized train dataset features: {'input_ids': List(Value('int32')), 'attention_mask': List(Value('int8')), 'labels': Value('int64')}\n",
      "\n",
      "Tokenized test dataset features: {'input_ids': List(Value('int32')), 'attention_mask': List(Value('int8')), 'labels': Value('int64')}\n",
      "\n",
      "Train dataset size: 1012\n",
      "Test dataset size: 254\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba0bdf1d77b43e99bab71b5b5ff9c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1012 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0467852d4e144508b0a07642c1b54087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/254 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_train_dataset 삭제됨\n",
      "cls_test_dataset 삭제됨\n",
      "AutoTokenizer 삭제됨\n",
      "DatasetDict 삭제됨\n",
      "load_tokenizer_from_local 삭제됨\n",
      "tokenize_function 삭제됨\n",
      "pd 삭제됨\n",
      "tokenized_cls_train_datasets 삭제됨\n",
      "tokenized_cls_eval_datasets 삭제됨\n",
      "tokenized_cls_datasets 삭제됨\n"
     ]
    }
   ],
   "source": [
    "# 데이터 셋 전처리\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import DatasetDict\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# 토크나이저 디스크에서 메모리로 로드\n",
    "def load_tokenizer_from_local(model_path: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = load_tokenizer_from_local(model_path)\n",
    "\n",
    "def tokenize_function(elements):\n",
    "    batch_tokenized = {'input_ids': [], 'attention_mask': [], 'labels': []}\n",
    "    \n",
    "    for i in range(len(elements['template'])):\n",
    "        template = elements['template'][i]\n",
    "        reason = elements['reject_reason'][i]\n",
    "        \n",
    "        if pd.isna(reason):\n",
    "            # reason이 없는 경우: [CLS] template [SEP]\n",
    "            tokenized = tokenizer(\n",
    "                text=template,\n",
    "                padding=False,\n",
    "                max_length=512,\n",
    "                truncation=True\n",
    "            )\n",
    "        else:\n",
    "            # reason이 있는 경우: [CLS] template [SEP] reason [SEP]\n",
    "            tokenized = tokenizer(\n",
    "                text=template,\n",
    "                text_pair=str(reason),\n",
    "                padding=False,\n",
    "                max_length=512,\n",
    "                truncation=True\n",
    "            )\n",
    "        \n",
    "        batch_tokenized['input_ids'].append(tokenized['input_ids'])\n",
    "        batch_tokenized['attention_mask'].append(tokenized['attention_mask'])\n",
    "        batch_tokenized['labels'].append(elements['is_approved'][i])\n",
    "    \n",
    "    return batch_tokenized\n",
    "\n",
    "# 데이터셋에 토큰화 함수 적용\n",
    "print(\"\\nApplying tokenization function to the dataset...\")\n",
    "tokenized_cls_train_datasets = cls_train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=cls_train_dataset.column_names\n",
    ")\n",
    "tokenized_cls_eval_datasets = cls_test_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=cls_test_dataset.column_names\n",
    ")\n",
    "\n",
    "tokenized_cls_datasets = DatasetDict({\n",
    "    \"train\": tokenized_cls_train_datasets,\n",
    "    \"eval\": tokenized_cls_eval_datasets\n",
    "})\n",
    "\n",
    "print(\"Tokenized train dataset features:\", tokenized_cls_train_datasets.features)\n",
    "print(\"\\nTokenized test dataset features:\", tokenized_cls_eval_datasets.features)\n",
    "print(f\"\\nTrain dataset size: {len(tokenized_cls_train_datasets)}\")\n",
    "print(f\"Test dataset size: {len(tokenized_cls_eval_datasets)}\")\n",
    "\n",
    "tokenized_path = \"./tokenized_datasets/\" + \"klue/bert-base\".replace(\"/\", \"--\")\n",
    "tokenized_cls_datasets.save_to_disk(tokenized_path)\n",
    "\n",
    "cleanup_globals([\"model_path\", \"tokenizer\", \"tokenized_path\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "execution_state": "idle",
   "id": "1d42036f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T01:37:09.016308Z",
     "iopub.status.busy": "2025-09-10T01:37:09.016061Z",
     "iopub.status.idle": "2025-09-10T01:37:09.172037Z",
     "shell.execute_reply": "2025-09-10T01:37:09.171447Z",
     "shell.execute_reply.started": "2025-09-10T01:37:09.016287Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 사용 중인 GPU 메모리: 0.00 GB\n",
      "현재 캐시된 GPU 메모리: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# GPU가 사용 가능한지 확인\n",
    "if torch.cuda.is_available():\n",
    "    # 현재 사용 중인 메모리 (바이트)\n",
    "    allocated_bytes = torch.cuda.memory_allocated(device=0)\n",
    "    # 캐시된 메모리 (바이트)\n",
    "    reserved_bytes = torch.cuda.memory_reserved(device=0)\n",
    "\n",
    "    # GB 단위로 변환\n",
    "    gb_factor = 1024 * 1024 * 1024\n",
    "    allocated_gb = allocated_bytes / gb_factor\n",
    "    reserved_gb = reserved_bytes / gb_factor\n",
    "\n",
    "    print(f\"현재 사용 중인 GPU 메모리: {allocated_gb:.2f} GB\")\n",
    "    print(f\"현재 캐시된 GPU 메모리: {reserved_gb:.2f} GB\")\n",
    "\n",
    "else:\n",
    "    print(\"GPU를 사용할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "execution_state": "idle",
   "id": "8cb2b49c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T01:37:15.962734Z",
     "iopub.status.busy": "2025-09-10T01:37:15.962400Z",
     "iopub.status.idle": "2025-09-10T01:37:16.170096Z",
     "shell.execute_reply": "2025-09-10T01:37:16.169563Z",
     "shell.execute_reply.started": "2025-09-10T01:37:15.962717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_path 삭제됨\n",
      "tokenizer 삭제됨\n",
      "tokenized_path 삭제됨\n",
      "allocated_bytes 삭제됨\n",
      "reserved_bytes 삭제됨\n",
      "gb_factor 삭제됨\n",
      "allocated_gb 삭제됨\n",
      "reserved_gb 삭제됨\n"
     ]
    }
   ],
   "source": [
    "cleanup_globals([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "execution_state": "idle",
   "id": "96c47227",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T01:37:28.047125Z",
     "iopub.status.busy": "2025-09-10T01:37:28.046910Z",
     "iopub.status.idle": "2025-09-10T01:37:32.371490Z",
     "shell.execute_reply": "2025-09-10T01:37:32.370814Z",
     "shell.execute_reply.started": "2025-09-10T01:37:28.047110Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./downloaded_model/klue--bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoTokenizer 삭제됨\n",
      "AutoModelForSequenceClassification 삭제됨\n",
      "model_path 삭제됨\n",
      "load_model_and_tokenizer_from_local 삭제됨\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_path = \"./downloaded_model/klue--bert-base\"\n",
    "\n",
    "def load_model_and_tokenizer_from_local(model_path: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_path,\n",
    "        local_files_only=True,\n",
    "        # classifier_dropout=0.2, # 과적합 발생하면 시도\n",
    "        num_labels=2,\n",
    "        id2label={0: \"Not Approved\", 1: \"Approved\"},\n",
    "        label2id={\"Not Approved\": 0, \"Approved\": 1},\n",
    "        problem_type=\"single_label_classification\"\n",
    "    )\n",
    "    return tokenizer, model\n",
    "\n",
    "tokenizer, model = load_model_and_tokenizer_from_local(model_path)\n",
    "\n",
    "cleanup_globals([\"tokenizer\", \"model\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "execution_state": "idle",
   "id": "0c290e60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T01:37:48.995874Z",
     "iopub.status.busy": "2025-09-10T01:37:48.995578Z",
     "iopub.status.idle": "2025-09-10T01:37:49.666746Z",
     "shell.execute_reply": "2025-09-10T01:37:49.666054Z",
     "shell.execute_reply.started": "2025-09-10T01:37:48.995852Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "tokenized_path = \"./tokenized_datasets/\" + \"klue/bert-base\".replace(\"/\", \"--\")\n",
    "tokenized_cls_datasets = load_from_disk(tokenized_path)\n",
    "tokenized_cls_train_datasets = tokenized_cls_datasets['train']\n",
    "tokenized_cls_eval_datasets = tokenized_cls_datasets['eval']\n",
    "\n",
    "# DataCollator 정의 - 텍스트 분류는 DataCollatorWithPadding 사용\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,\n",
    "    padding='longest', # 배치 내에서 가장 긴 시퀀스에 맞춰 패딩\n",
    "    pad_to_multiple_of=8, # 학습 속도를 약간 높여줌\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "execution_state": "idle",
   "id": "5972f931",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T01:41:43.496161Z",
     "iopub.status.busy": "2025-09-10T01:41:43.495827Z",
     "iopub.status.idle": "2025-09-10T01:45:22.450762Z",
     "shell.execute_reply": "2025-09-10T01:45:22.450001Z",
     "shell.execute_reply.started": "2025-09-10T01:41:43.496144Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">klue-bert-base</strong> at: <a href='https://wandb.ai/dr-hong/dr-hong/runs/777rk1xi' target=\"_blank\">https://wandb.ai/dr-hong/dr-hong/runs/777rk1xi</a><br> View project at: <a href='https://wandb.ai/dr-hong/dr-hong' target=\"_blank\">https://wandb.ai/dr-hong/dr-hong</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250910_014017-777rk1xi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/custom-file-systems/s3/shared/wandb/run-20250910_014143-ygo2xa8z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dr-hong/dr-hong/runs/ygo2xa8z' target=\"_blank\">klue-bert-base</a></strong> to <a href='https://wandb.ai/dr-hong/dr-hong' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dr-hong/dr-hong' target=\"_blank\">https://wandb.ai/dr-hong/dr-hong</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dr-hong/dr-hong/runs/ygo2xa8z' target=\"_blank\">https://wandb.ai/dr-hong/dr-hong/runs/ygo2xa8z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/transformers/training_args.py:1818: FutureWarning: `torchdynamo` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `torch_compile_backend` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/torch/_inductor/lowering.py:7095: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/torch/_inductor/lowering.py:7095: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 02:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.091000</td>\n",
       "      <td>0.216205</td>\n",
       "      <td>0.952756</td>\n",
       "      <td>0.947706</td>\n",
       "      <td>0.955168</td>\n",
       "      <td>0.952756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.105700</td>\n",
       "      <td>0.155157</td>\n",
       "      <td>0.960630</td>\n",
       "      <td>0.957276</td>\n",
       "      <td>0.962320</td>\n",
       "      <td>0.960630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.071500</td>\n",
       "      <td>0.231053</td>\n",
       "      <td>0.956693</td>\n",
       "      <td>0.952544</td>\n",
       "      <td>0.958729</td>\n",
       "      <td>0.956693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.124500</td>\n",
       "      <td>0.193842</td>\n",
       "      <td>0.956693</td>\n",
       "      <td>0.952544</td>\n",
       "      <td>0.958729</td>\n",
       "      <td>0.956693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.058300</td>\n",
       "      <td>0.154294</td>\n",
       "      <td>0.960630</td>\n",
       "      <td>0.957276</td>\n",
       "      <td>0.962320</td>\n",
       "      <td>0.960630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.043900</td>\n",
       "      <td>0.160847</td>\n",
       "      <td>0.960630</td>\n",
       "      <td>0.957276</td>\n",
       "      <td>0.962320</td>\n",
       "      <td>0.960630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.029000</td>\n",
       "      <td>0.176957</td>\n",
       "      <td>0.960630</td>\n",
       "      <td>0.957276</td>\n",
       "      <td>0.962320</td>\n",
       "      <td>0.960630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>0.229782</td>\n",
       "      <td>0.960630</td>\n",
       "      <td>0.957276</td>\n",
       "      <td>0.962320</td>\n",
       "      <td>0.960630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.059200</td>\n",
       "      <td>0.231531</td>\n",
       "      <td>0.960630</td>\n",
       "      <td>0.957276</td>\n",
       "      <td>0.962320</td>\n",
       "      <td>0.960630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0910 01:42:21.333000 3842 torch/fx/experimental/symbolic_shapes.py:6823] [0/1] _maybe_guard_rel() was called on non-relation expression Eq(s16, 1) | Eq(s27, s16)\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/torch/_inductor/lowering.py:7095: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "W0910 01:43:14.155000 3842 torch/fx/experimental/symbolic_shapes.py:6823] [0/2] _maybe_guard_rel() was called on non-relation expression Eq(s52, s92) | Eq(s92, 1)\n",
      "W0910 01:43:14.158000 3842 torch/fx/experimental/symbolic_shapes.py:6823] [0/2] _maybe_guard_rel() was called on non-relation expression Eq(s16, 1) | Eq(s27, s16)\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/torch/_inductor/lowering.py:7095: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁█▅▅█████▁</td></tr><tr><td>eval/f1</td><td>▁█▅▅█████▁</td></tr><tr><td>eval/loss</td><td>▇▁█▅▁▂▃██▇</td></tr><tr><td>eval/precision</td><td>▁█▄▄█████▁</td></tr><tr><td>eval/recall</td><td>▁█▅▅█████▁</td></tr><tr><td>eval/runtime</td><td>▇▁▁▁▁▁▁▁▁█</td></tr><tr><td>eval/samples_per_second</td><td>▁████████▁</td></tr><tr><td>eval/steps_per_second</td><td>▁████████▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>+3</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.95276</td></tr><tr><td>eval/f1</td><td>0.94771</td></tr><tr><td>eval/loss</td><td>0.2162</td></tr><tr><td>eval/precision</td><td>0.95517</td></tr><tr><td>eval/recall</td><td>0.95276</td></tr><tr><td>eval/runtime</td><td>0.745</td></tr><tr><td>eval/samples_per_second</td><td>340.923</td></tr><tr><td>eval/steps_per_second</td><td>10.738</td></tr><tr><td>total_flos</td><td>322376820579840.0</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>+8</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">klue-bert-base</strong> at: <a href='https://wandb.ai/dr-hong/dr-hong/runs/ygo2xa8z' target=\"_blank\">https://wandb.ai/dr-hong/dr-hong/runs/ygo2xa8z</a><br> View project at: <a href='https://wandb.ai/dr-hong/dr-hong' target=\"_blank\">https://wandb.ai/dr-hong/dr-hong</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250910_014143-ygo2xa8z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# wandb 초기화\n",
    "wandb.init(\n",
    "    entity=\"dr-hong\",\n",
    "    project=\"dr-hong\",\n",
    "    name=\"klue-bert-base\",\n",
    "    config={\n",
    "        \"learning_rate\": 3e-5,\n",
    "        \"epochs\": 3,\n",
    "        \"batch_size\": 32,\n",
    "        \"model_name\": \"klue/bert-base\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# compute_metrics 함수 정의. bert 에 적합한 평가지표 선택: accuracy, f1, precision, recall\n",
    "def compute_metrics(eval_pred, batch_eval_metrics=False):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = predictions.argmax(axis=1)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# 학습 인자 정의\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/\" + \"klue/bert-base\".replace(\"/\", \"--\"),\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=1,\n",
    "    eval_delay=32,\n",
    "    learning_rate=3e-5, # differential learning rate 적용 고려해 볼 수 있음\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_dir=\"./logs/\" + \"klue/bert-base\".replace(\"/\", \"--\"),\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=16,\n",
    "    save_total_limit=1,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    eval_steps=8,\n",
    "    dataloader_num_workers=4,\n",
    "    disable_tqdm=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"all\",\n",
    "    dataloader_persistent_workers=True,\n",
    "    resume_from_checkpoint=\"./results/\" + \"klue/bert-base\".replace(\"/\", \"--\"),\n",
    "    gradient_checkpointing=True,\n",
    "    auto_find_batch_size=True,\n",
    "    torchdynamo=\"inductor\",\n",
    "    torch_compile=True,\n",
    "    torch_compile_backend=\"inductor\",\n",
    "    torch_compile_mode=\"default\",\n",
    ")\n",
    "\n",
    "# Trainer 설정\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_cls_train_datasets,\n",
    "    eval_dataset=tokenized_cls_eval_datasets,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "try:\n",
    "    train_output = trainer.train()\n",
    "\n",
    "    eval_results = trainer.evaluate()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"학습 중 오류 발생: {str(e)}\")\n",
    "    raise e\n",
    "\n",
    "# 최종 모델 저장\n",
    "model.save_pretrained(\"./finetuned_model/\" + \"klue/bert-base\".replace(\"/\", \"--\"))\n",
    "\n",
    "# wandb 종료\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "execution_state": "idle",
   "id": "2b56a4d2-dd9a-41ae-90f1-60441424a96c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T01:56:29.001973Z",
     "iopub.status.busy": "2025-09-10T01:56:29.001745Z",
     "iopub.status.idle": "2025-09-10T01:56:33.508270Z",
     "shell.execute_reply": "2025-09-10T01:56:33.507569Z",
     "shell.execute_reply.started": "2025-09-10T01:56:29.001958Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template: {\"title\": \"회사소개서 발송\", \"text\": \"안녕하세요 #{수신자명}님, 저희 회사를 소개드립니다.\"}\n",
      "Prediction: Approved\n",
      "Confidence: 0.9935\n",
      "Probabilities: {'Not Approved': 0.0064859273843467236, 'Approved': 0.9935140609741211}\n"
     ]
    }
   ],
   "source": [
    "# 추론 테스트\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "\n",
    "# 학습된 모델 로드\n",
    "base_model_path = \"./downloaded_model/\" + \"klue/bert-base\".replace(\"/\", \"--\")\n",
    "finetuned_model_path = \"./finetuned_model/\" + \"klue/bert-base\".replace(\"/\", \"--\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(finetuned_model_path)\n",
    "model.eval()\n",
    "\n",
    "def predict_template(template, reject_reason=None):\n",
    "    # 토큰화\n",
    "    if reject_reason and not pd.isna(reject_reason):\n",
    "        inputs = tokenizer(\n",
    "            text=template,\n",
    "            text_pair=str(reject_reason),\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    else:\n",
    "        inputs = tokenizer(\n",
    "            text=template,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    \n",
    "    # 추론\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(predictions, dim=-1).item()\n",
    "        confidence = predictions[0][predicted_class].item()\n",
    "    \n",
    "    return {\n",
    "        \"prediction\": \"Approved\" if predicted_class == 1 else \"Not Approved\",\n",
    "        \"confidence\": confidence,\n",
    "        \"probabilities\": {\n",
    "            \"Not Approved\": predictions[0][0].item(),\n",
    "            \"Approved\": predictions[0][1].item()\n",
    "        }\n",
    "    }\n",
    "\n",
    "# 테스트 예시\n",
    "test_template = '{\"title\": \"회사소개서 발송\", \"text\": \"안녕하세요 #{수신자명}님, 저희 회사를 소개드립니다.\"}'\n",
    "result = predict_template(test_template)\n",
    "print(f\"Template: {test_template}\")\n",
    "print(f\"Prediction: {result['prediction']}\")\n",
    "print(f\"Confidence: {result['confidence']:.4f}\")\n",
    "print(f\"Probabilities: {result['probabilities']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dr-hong-pr",
   "language": "python",
   "name": "dr-hong-pr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
