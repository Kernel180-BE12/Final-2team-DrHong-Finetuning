{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "execution_state": "idle",
   "id": "0d5a46bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T01:18:29.146303Z",
     "iopub.status.busy": "2025-09-10T01:18:29.146059Z",
     "iopub.status.idle": "2025-09-10T01:18:34.264971Z",
     "shell.execute_reply": "2025-09-10T01:18:34.264308Z",
     "shell.execute_reply.started": "2025-09-10T01:18:29.146283Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "def cleanup_globals(vars_to_keep: list):\n",
    "    global_vars = list(globals().keys())\n",
    "    protected_vars = ['In', 'Out', 'get_ipython', 'exit', 'quit', 'gc', 'torch', 'cleanup_globals']\n",
    "\n",
    "    for var in global_vars:\n",
    "        if var not in vars_to_keep and not var.startswith('_') and var not in protected_vars:\n",
    "            try:\n",
    "                del globals()[var]\n",
    "                print(f\"{var} ì‚­ì œë¨\")\n",
    "            except:\n",
    "                continue\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "execution_state": "idle",
   "id": "cc8de442",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T01:19:39.307896Z",
     "iopub.status.busy": "2025-09-10T01:19:39.307643Z",
     "iopub.status.idle": "2025-09-10T01:19:40.872448Z",
     "shell.execute_reply": "2025-09-10T01:19:40.871740Z",
     "shell.execute_reply.started": "2025-09-10T01:19:39.307879Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ë©”ëª¨ë¦¬ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.\n",
      "íŒŒì¼ì„ pandas dataframe ë¡œ ë³€í™˜\n",
      "ì›ë³¸ ë°ì´í„° ìƒìœ„ 5ê°œ\n",
      "<bound method NDFrame.head of                                                template  is_approved  \\\n",
      "0     {\"title\": \"íšŒì‚¬ì†Œê°œì„œ ë°œì†¡\", \"text\": \"ì•ˆë…•í•˜ì„¸ìš” #{ìˆ˜ì‹ ìëª…}ë‹˜,...            1   \n",
      "1     {\"title\": \"ì„œë¹„ìŠ¤ ì†Œê°œì„œ ë°œì†¡\", \"text\": \"ì•ˆë…•í•˜ì„¸ìš” #{ìˆ˜ì‹ ìëª…}...            1   \n",
      "2     {\"title\": \"(ì „ìš©) ê°•ì˜ ì¼ì • ì•ˆë‚´ / í™”ì¼€í„°\", \"text\": \"ì•ˆë…•í•˜ì„¸...            1   \n",
      "3     {\"title\": \"(ê³µìš©) í›„ê¸° ì‘ì„± ìš”ì²­_ì´ë¯¸ì§€í˜•_01\", \"text\": \"[í…œ...            1   \n",
      "4     {\"title\": \"(ê³µìš©) ì¸ë³´ì´ìŠ¤ ì•Œë¦¼_ì´ë¯¸ì§€í˜•_01\", \"text\": \"â–  #...            1   \n",
      "...                                                 ...          ...   \n",
      "1261  {\"title\": \"(ê³µìš©) ì ë¦½ê¸ˆ ì†Œë©¸ ì•ˆë‚´_ì´ë¯¸ì§€í˜•_01\", \"text\": \"ì•ˆ...            0   \n",
      "1262  {\"title\": \"(ì „ìš©) AS ì•ˆë‚´_ê¸°ë³¸í˜•_01 / ë¼ë²¨ë¥´\", \"text\": \"...            0   \n",
      "1263  {\"title\": \"(ê³µìš©) ë¬¸ì„œ ë„ì°© ì•Œë¦¼_ì´ë¯¸ì§€í˜•_08\", \"text\": \"â–  ...            0   \n",
      "1264  {\"title\": \"(ê³µìš©) ë§í¬_ ë¬¸ì„œ ë„ì°© ì•Œë¦¼_ì´ë¯¸ì§€í˜•_01\", \"text\":...            0   \n",
      "1265  {\"title\": \"(ê³µìš©) ë§í¬_ë¬¸ì„œ ë„ì°© ì•Œë¦¼_ì´ë¯¸ì§€í˜•_01\", \"text\": ...            0   \n",
      "\n",
      "     reject_reason  \n",
      "0              NaN  \n",
      "1              NaN  \n",
      "2              NaN  \n",
      "3              NaN  \n",
      "4              NaN  \n",
      "...            ...  \n",
      "1261           NaN  \n",
      "1262           NaN  \n",
      "1263           NaN  \n",
      "1264           NaN  \n",
      "1265           NaN  \n",
      "\n",
      "[1266 rows x 3 columns]>\n",
      "\n",
      "Hugging Face Dataset ìœ¼ë¡œ ë³€í™˜\n",
      "ìµœì¢… ë¶„í• ëœ ë°ì´í„° ì…‹\n",
      "Dataset({\n",
      "    features: ['template', 'is_approved', 'reject_reason'],\n",
      "    num_rows: 1012\n",
      "})\n",
      "Dataset({\n",
      "    features: ['template', 'is_approved', 'reject_reason'],\n",
      "    num_rows: 254\n",
      "})\n",
      "open ì‚­ì œë¨\n",
      "boto3 ì‚­ì œë¨\n",
      "pd ì‚­ì œë¨\n",
      "io ì‚­ì œë¨\n",
      "Dataset ì‚­ì œë¨\n",
      "credentials_df ì‚­ì œë¨\n",
      "aws_access_key_id ì‚­ì œë¨\n",
      "aws_secret_access_key ì‚­ì œë¨\n",
      "bucket_name ì‚­ì œë¨\n",
      "file_key ì‚­ì œë¨\n",
      "s3_client ì‚­ì œë¨\n",
      "file_content ì‚­ì œë¨\n",
      "df ì‚­ì œë¨\n",
      "cls_dataset ì‚­ì œë¨\n",
      "cls_train_test_dataset ì‚­ì œë¨\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "from datasets import Dataset\n",
    "\n",
    "try:\n",
    "    credentials_df = pd.read_csv('./ganghyun-dev_accessKeys.csv')\n",
    "\n",
    "    if not credentials_df.empty:\n",
    "        aws_access_key_id = credentials_df['Access key ID'].iloc[0].strip()\n",
    "        aws_secret_access_key = credentials_df['Secret access key'].iloc[0].strip()\n",
    "    else:\n",
    "        print(\"Error: 'aws_credentials.csv' is empty.\")\n",
    "        exit()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'aws_credentials.csv' not found in Drive.\")\n",
    "    print(\"Please create a file named 'aws_credentials.csv' in your Google Drive with your AWS credentials.\")\n",
    "    exit()\n",
    "except KeyError:\n",
    "    print(\"Error: 'Access key ID' or 'Secret access key' column not found in 'aws_credentials.csv'.\")\n",
    "    print(\"Please ensure your CSV file has these columns.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading AWS credentials from CSV: {e}\")\n",
    "    exit()\n",
    "\n",
    "bucket_name = \"dr.hong-s3\"\n",
    "\n",
    "file_key = \"dataset/origin_template_classification_dataset.xlsx\"\n",
    "\n",
    "s3_client = boto3.client('s3',\n",
    "                         aws_access_key_id=aws_access_key_id,\n",
    "                         aws_secret_access_key=aws_secret_access_key)\n",
    "\n",
    "try:\n",
    "    file_content = s3_client.get_object(Bucket=bucket_name, Key=file_key)['Body'].read()\n",
    "    print(\"íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ë©”ëª¨ë¦¬ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"S3ì—ì„œ íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ì—‘ì…€ íŒŒì¼ì„ pandas datafrome ìœ¼ë¡œ ë³€í™˜\n",
    "print(\"íŒŒì¼ì„ pandas dataframe ë¡œ ë³€í™˜\")\n",
    "df = pd.read_excel(io.BytesIO(file_content))\n",
    "\n",
    "print(\"ì›ë³¸ ë°ì´í„° ìƒìœ„ 5ê°œ\")\n",
    "print(df.head)\n",
    "print()\n",
    "\n",
    "print(\"Hugging Face Dataset ìœ¼ë¡œ ë³€í™˜\")\n",
    "cls_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "cls_train_test_dataset = cls_dataset.train_test_split(test_size=0.2, shuffle=True)\n",
    "cls_train_dataset = cls_train_test_dataset[\"train\"]\n",
    "cls_test_dataset = cls_train_test_dataset[\"test\"]\n",
    "\n",
    "print(\"ìµœì¢… ë¶„í• ëœ ë°ì´í„° ì…‹\")\n",
    "print(cls_train_dataset)\n",
    "print(cls_test_dataset)\n",
    "\n",
    "cleanup_globals([\"cls_train_dataset\", \"cls_test_dataset\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "execution_state": "idle",
   "id": "5ca848df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T01:19:55.835642Z",
     "iopub.status.busy": "2025-09-10T01:19:55.835412Z",
     "iopub.status.idle": "2025-09-10T01:21:09.014753Z",
     "shell.execute_reply": "2025-09-10T01:21:09.014115Z",
     "shell.execute_reply.started": "2025-09-10T01:19:55.835625Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'klue/bert-base' ëª¨ë¸ì„ './downloaded_model/klue--bert-base' ê²½ë¡œì— ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f5e7fd66b0d4732a81f362237f72550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b8f9692fa164b2fadc39ab80aaca06f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/425 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c67c86255fc447db42478a9224af723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b4c3526249e4a579e5e722ced09bfab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1106fe5927347249db6f3128ad7eb89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/289 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a66213d6a24a459e809ab2e8dfd8f488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a0bcee7e18451db5bc8224416412d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/744 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d2b2c3c4eb54d5fba86b162f0f66251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ab46ac53864b87baae69aaf03820cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a898597cd6294c2d85e7fc4420e77d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!\n",
      "os ì‚­ì œë¨\n",
      "snapshot_download ì‚­ì œë¨\n",
      "RepositoryNotFoundError ì‚­ì œë¨\n",
      "download_model_snapshot ì‚­ì œë¨\n",
      "checkpoint ì‚­ì œë¨\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "from huggingface_hub import snapshot_download\n",
    "from huggingface_hub.utils import RepositoryNotFoundError\n",
    "\n",
    "def download_model_snapshot(model_id: str, local_dir: str) -> str:\n",
    "    print(f\"'{model_id}' ëª¨ë¸ì„ '{local_dir}' ê²½ë¡œì— ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤...\")\n",
    "    try:\n",
    "        # snapshot_downloadëŠ” ì•Œì•„ì„œ ê¸°ì¡´ íŒŒì¼ì„ ì²´í¬í•˜ê³  í•„ìš”í•œ ê²ƒë§Œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "        model_path = snapshot_download(\n",
    "            repo_id=model_id,\n",
    "            local_dir=local_dir\n",
    "            # resume_download=True, # ê¸°ë³¸ê°’ì´ Trueì´ë¯€ë¡œ ëª…ì‹œí•˜ì§€ ì•Šì•„ë„ ë¨\n",
    "        )\n",
    "        print(\"âœ… ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "        return model_path\n",
    "    except RepositoryNotFoundError:\n",
    "        print(f\"âŒ ì˜¤ë¥˜: ëª¨ë¸ ID '{model_id}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")\n",
    "        return None\n",
    "\n",
    "# ì‹¤í–‰\n",
    "checkpoint = \"klue/bert-base\" # CC-BY-SA-4.0 ë¼ì´ì„ ìŠ¤ ì œì•½ ìˆìŒ\n",
    "model_path = download_model_snapshot(checkpoint, \"./downloaded_model/\" + checkpoint.replace(\"/\", \"--\"))\n",
    "\n",
    "cleanup_globals([\"cls_train_dataset\", \"cls_test_dataset\", \"model_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "execution_state": "idle",
   "id": "5156a67d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T01:35:44.142837Z",
     "iopub.status.busy": "2025-09-10T01:35:44.142582Z",
     "iopub.status.idle": "2025-09-10T01:35:46.163985Z",
     "shell.execute_reply": "2025-09-10T01:35:46.163423Z",
     "shell.execute_reply.started": "2025-09-10T01:35:44.142821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying tokenization function to the dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb352b663abf49eea711caea972847e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1012 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c305ca1194b45c29030822744c03335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/254 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized train dataset features: {'input_ids': List(Value('int32')), 'attention_mask': List(Value('int8')), 'labels': Value('int64')}\n",
      "\n",
      "Tokenized test dataset features: {'input_ids': List(Value('int32')), 'attention_mask': List(Value('int8')), 'labels': Value('int64')}\n",
      "\n",
      "Train dataset size: 1012\n",
      "Test dataset size: 254\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba0bdf1d77b43e99bab71b5b5ff9c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1012 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0467852d4e144508b0a07642c1b54087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/254 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_train_dataset ì‚­ì œë¨\n",
      "cls_test_dataset ì‚­ì œë¨\n",
      "AutoTokenizer ì‚­ì œë¨\n",
      "DatasetDict ì‚­ì œë¨\n",
      "load_tokenizer_from_local ì‚­ì œë¨\n",
      "tokenize_function ì‚­ì œë¨\n",
      "pd ì‚­ì œë¨\n",
      "tokenized_cls_train_datasets ì‚­ì œë¨\n",
      "tokenized_cls_eval_datasets ì‚­ì œë¨\n",
      "tokenized_cls_datasets ì‚­ì œë¨\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„° ì…‹ ì „ì²˜ë¦¬\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import DatasetDict\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë””ìŠ¤í¬ì—ì„œ ë©”ëª¨ë¦¬ë¡œ ë¡œë“œ\n",
    "def load_tokenizer_from_local(model_path: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = load_tokenizer_from_local(model_path)\n",
    "\n",
    "def tokenize_function(elements):\n",
    "    batch_tokenized = {'input_ids': [], 'attention_mask': [], 'labels': []}\n",
    "    \n",
    "    for i in range(len(elements['template'])):\n",
    "        template = elements['template'][i]\n",
    "        reason = elements['reject_reason'][i]\n",
    "        \n",
    "        if pd.isna(reason):\n",
    "            # reasonì´ ì—†ëŠ” ê²½ìš°: [CLS] template [SEP]\n",
    "            tokenized = tokenizer(\n",
    "                text=template,\n",
    "                padding=False,\n",
    "                max_length=512,\n",
    "                truncation=True\n",
    "            )\n",
    "        else:\n",
    "            # reasonì´ ìˆëŠ” ê²½ìš°: [CLS] template [SEP] reason [SEP]\n",
    "            tokenized = tokenizer(\n",
    "                text=template,\n",
    "                text_pair=str(reason),\n",
    "                padding=False,\n",
    "                max_length=512,\n",
    "                truncation=True\n",
    "            )\n",
    "        \n",
    "        batch_tokenized['input_ids'].append(tokenized['input_ids'])\n",
    "        batch_tokenized['attention_mask'].append(tokenized['attention_mask'])\n",
    "        batch_tokenized['labels'].append(elements['is_approved'][i])\n",
    "    \n",
    "    return batch_tokenized\n",
    "\n",
    "# ë°ì´í„°ì…‹ì— í† í°í™” í•¨ìˆ˜ ì ìš©\n",
    "print(\"\\nApplying tokenization function to the dataset...\")\n",
    "tokenized_cls_train_datasets = cls_train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=cls_train_dataset.column_names\n",
    ")\n",
    "tokenized_cls_eval_datasets = cls_test_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=cls_test_dataset.column_names\n",
    ")\n",
    "\n",
    "tokenized_cls_datasets = DatasetDict({\n",
    "    \"train\": tokenized_cls_train_datasets,\n",
    "    \"eval\": tokenized_cls_eval_datasets\n",
    "})\n",
    "\n",
    "print(\"Tokenized train dataset features:\", tokenized_cls_train_datasets.features)\n",
    "print(\"\\nTokenized test dataset features:\", tokenized_cls_eval_datasets.features)\n",
    "print(f\"\\nTrain dataset size: {len(tokenized_cls_train_datasets)}\")\n",
    "print(f\"Test dataset size: {len(tokenized_cls_eval_datasets)}\")\n",
    "\n",
    "tokenized_path = \"./tokenized_datasets/\" + \"klue/bert-base\".replace(\"/\", \"--\")\n",
    "tokenized_cls_datasets.save_to_disk(tokenized_path)\n",
    "\n",
    "cleanup_globals([\"model_path\", \"tokenizer\", \"tokenized_path\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "execution_state": "idle",
   "id": "1d42036f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T01:37:09.016308Z",
     "iopub.status.busy": "2025-09-10T01:37:09.016061Z",
     "iopub.status.idle": "2025-09-10T01:37:09.172037Z",
     "shell.execute_reply": "2025-09-10T01:37:09.171447Z",
     "shell.execute_reply.started": "2025-09-10T01:37:09.016287Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ GPU ë©”ëª¨ë¦¬: 0.00 GB\n",
      "í˜„ì¬ ìºì‹œëœ GPU ë©”ëª¨ë¦¬: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# GPUê°€ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸\n",
    "if torch.cuda.is_available():\n",
    "    # í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë©”ëª¨ë¦¬ (ë°”ì´íŠ¸)\n",
    "    allocated_bytes = torch.cuda.memory_allocated(device=0)\n",
    "    # ìºì‹œëœ ë©”ëª¨ë¦¬ (ë°”ì´íŠ¸)\n",
    "    reserved_bytes = torch.cuda.memory_reserved(device=0)\n",
    "\n",
    "    # GB ë‹¨ìœ„ë¡œ ë³€í™˜\n",
    "    gb_factor = 1024 * 1024 * 1024\n",
    "    allocated_gb = allocated_bytes / gb_factor\n",
    "    reserved_gb = reserved_bytes / gb_factor\n",
    "\n",
    "    print(f\"í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ GPU ë©”ëª¨ë¦¬: {allocated_gb:.2f} GB\")\n",
    "    print(f\"í˜„ì¬ ìºì‹œëœ GPU ë©”ëª¨ë¦¬: {reserved_gb:.2f} GB\")\n",
    "\n",
    "else:\n",
    "    print(\"GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "execution_state": "idle",
   "id": "8cb2b49c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T01:37:15.962734Z",
     "iopub.status.busy": "2025-09-10T01:37:15.962400Z",
     "iopub.status.idle": "2025-09-10T01:37:16.170096Z",
     "shell.execute_reply": "2025-09-10T01:37:16.169563Z",
     "shell.execute_reply.started": "2025-09-10T01:37:15.962717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_path ì‚­ì œë¨\n",
      "tokenizer ì‚­ì œë¨\n",
      "tokenized_path ì‚­ì œë¨\n",
      "allocated_bytes ì‚­ì œë¨\n",
      "reserved_bytes ì‚­ì œë¨\n",
      "gb_factor ì‚­ì œë¨\n",
      "allocated_gb ì‚­ì œë¨\n",
      "reserved_gb ì‚­ì œë¨\n"
     ]
    }
   ],
   "source": [
    "cleanup_globals([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "execution_state": "idle",
   "id": "96c47227",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T01:37:28.047125Z",
     "iopub.status.busy": "2025-09-10T01:37:28.046910Z",
     "iopub.status.idle": "2025-09-10T01:37:32.371490Z",
     "shell.execute_reply": "2025-09-10T01:37:32.370814Z",
     "shell.execute_reply.started": "2025-09-10T01:37:28.047110Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./downloaded_model/klue--bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoTokenizer ì‚­ì œë¨\n",
      "AutoModelForSequenceClassification ì‚­ì œë¨\n",
      "model_path ì‚­ì œë¨\n",
      "load_model_and_tokenizer_from_local ì‚­ì œë¨\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_path = \"./downloaded_model/klue--bert-base\"\n",
    "\n",
    "def load_model_and_tokenizer_from_local(model_path: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_path,\n",
    "        local_files_only=True,\n",
    "        # classifier_dropout=0.2, # ê³¼ì í•© ë°œìƒí•˜ë©´ ì‹œë„\n",
    "        num_labels=2,\n",
    "        id2label={0: \"Not Approved\", 1: \"Approved\"},\n",
    "        label2id={\"Not Approved\": 0, \"Approved\": 1},\n",
    "        problem_type=\"single_label_classification\"\n",
    "    )\n",
    "    return tokenizer, model\n",
    "\n",
    "tokenizer, model = load_model_and_tokenizer_from_local(model_path)\n",
    "\n",
    "cleanup_globals([\"tokenizer\", \"model\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "execution_state": "idle",
   "id": "0c290e60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T01:37:48.995874Z",
     "iopub.status.busy": "2025-09-10T01:37:48.995578Z",
     "iopub.status.idle": "2025-09-10T01:37:49.666746Z",
     "shell.execute_reply": "2025-09-10T01:37:49.666054Z",
     "shell.execute_reply.started": "2025-09-10T01:37:48.995852Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "tokenized_path = \"./tokenized_datasets/\" + \"klue/bert-base\".replace(\"/\", \"--\")\n",
    "tokenized_cls_datasets = load_from_disk(tokenized_path)\n",
    "tokenized_cls_train_datasets = tokenized_cls_datasets['train']\n",
    "tokenized_cls_eval_datasets = tokenized_cls_datasets['eval']\n",
    "\n",
    "# DataCollator ì •ì˜ - í…ìŠ¤íŠ¸ ë¶„ë¥˜ëŠ” DataCollatorWithPadding ì‚¬ìš©\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,\n",
    "    padding='longest', # ë°°ì¹˜ ë‚´ì—ì„œ ê°€ì¥ ê¸´ ì‹œí€€ìŠ¤ì— ë§ì¶° íŒ¨ë”©\n",
    "    pad_to_multiple_of=8, # í•™ìŠµ ì†ë„ë¥¼ ì•½ê°„ ë†’ì—¬ì¤Œ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "execution_state": "idle",
   "id": "5972f931",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T01:41:43.496161Z",
     "iopub.status.busy": "2025-09-10T01:41:43.495827Z",
     "iopub.status.idle": "2025-09-10T01:45:22.450762Z",
     "shell.execute_reply": "2025-09-10T01:45:22.450001Z",
     "shell.execute_reply.started": "2025-09-10T01:41:43.496144Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">klue-bert-base</strong> at: <a href='https://wandb.ai/dr-hong/dr-hong/runs/777rk1xi' target=\"_blank\">https://wandb.ai/dr-hong/dr-hong/runs/777rk1xi</a><br> View project at: <a href='https://wandb.ai/dr-hong/dr-hong' target=\"_blank\">https://wandb.ai/dr-hong/dr-hong</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250910_014017-777rk1xi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/custom-file-systems/s3/shared/wandb/run-20250910_014143-ygo2xa8z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dr-hong/dr-hong/runs/ygo2xa8z' target=\"_blank\">klue-bert-base</a></strong> to <a href='https://wandb.ai/dr-hong/dr-hong' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dr-hong/dr-hong' target=\"_blank\">https://wandb.ai/dr-hong/dr-hong</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dr-hong/dr-hong/runs/ygo2xa8z' target=\"_blank\">https://wandb.ai/dr-hong/dr-hong/runs/ygo2xa8z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/transformers/training_args.py:1818: FutureWarning: `torchdynamo` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use `torch_compile_backend` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/torch/_inductor/lowering.py:7095: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/torch/_inductor/lowering.py:7095: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 02:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.091000</td>\n",
       "      <td>0.216205</td>\n",
       "      <td>0.952756</td>\n",
       "      <td>0.947706</td>\n",
       "      <td>0.955168</td>\n",
       "      <td>0.952756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.105700</td>\n",
       "      <td>0.155157</td>\n",
       "      <td>0.960630</td>\n",
       "      <td>0.957276</td>\n",
       "      <td>0.962320</td>\n",
       "      <td>0.960630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.071500</td>\n",
       "      <td>0.231053</td>\n",
       "      <td>0.956693</td>\n",
       "      <td>0.952544</td>\n",
       "      <td>0.958729</td>\n",
       "      <td>0.956693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.124500</td>\n",
       "      <td>0.193842</td>\n",
       "      <td>0.956693</td>\n",
       "      <td>0.952544</td>\n",
       "      <td>0.958729</td>\n",
       "      <td>0.956693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.058300</td>\n",
       "      <td>0.154294</td>\n",
       "      <td>0.960630</td>\n",
       "      <td>0.957276</td>\n",
       "      <td>0.962320</td>\n",
       "      <td>0.960630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.043900</td>\n",
       "      <td>0.160847</td>\n",
       "      <td>0.960630</td>\n",
       "      <td>0.957276</td>\n",
       "      <td>0.962320</td>\n",
       "      <td>0.960630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.029000</td>\n",
       "      <td>0.176957</td>\n",
       "      <td>0.960630</td>\n",
       "      <td>0.957276</td>\n",
       "      <td>0.962320</td>\n",
       "      <td>0.960630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>0.229782</td>\n",
       "      <td>0.960630</td>\n",
       "      <td>0.957276</td>\n",
       "      <td>0.962320</td>\n",
       "      <td>0.960630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.059200</td>\n",
       "      <td>0.231531</td>\n",
       "      <td>0.960630</td>\n",
       "      <td>0.957276</td>\n",
       "      <td>0.962320</td>\n",
       "      <td>0.960630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0910 01:42:21.333000 3842 torch/fx/experimental/symbolic_shapes.py:6823] [0/1] _maybe_guard_rel() was called on non-relation expression Eq(s16, 1) | Eq(s27, s16)\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/torch/_inductor/lowering.py:7095: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "W0910 01:43:14.155000 3842 torch/fx/experimental/symbolic_shapes.py:6823] [0/2] _maybe_guard_rel() was called on non-relation expression Eq(s52, s92) | Eq(s92, 1)\n",
      "W0910 01:43:14.158000 3842 torch/fx/experimental/symbolic_shapes.py:6823] [0/2] _maybe_guard_rel() was called on non-relation expression Eq(s16, 1) | Eq(s27, s16)\n",
      "/home/sagemaker-user/.cache/pypoetry/virtualenvs/dr-hong-pr-60wuyk-o-py3.11/lib/python3.11/site-packages/torch/_inductor/lowering.py:7095: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>â–â–ˆâ–…â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–</td></tr><tr><td>eval/f1</td><td>â–â–ˆâ–…â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–</td></tr><tr><td>eval/loss</td><td>â–‡â–â–ˆâ–…â–â–‚â–ƒâ–ˆâ–ˆâ–‡</td></tr><tr><td>eval/precision</td><td>â–â–ˆâ–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–</td></tr><tr><td>eval/recall</td><td>â–â–ˆâ–…â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–</td></tr><tr><td>eval/runtime</td><td>â–‡â–â–â–â–â–â–â–â–â–ˆ</td></tr><tr><td>eval/samples_per_second</td><td>â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–</td></tr><tr><td>eval/steps_per_second</td><td>â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–</td></tr><tr><td>train/epoch</td><td>â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>+3</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.95276</td></tr><tr><td>eval/f1</td><td>0.94771</td></tr><tr><td>eval/loss</td><td>0.2162</td></tr><tr><td>eval/precision</td><td>0.95517</td></tr><tr><td>eval/recall</td><td>0.95276</td></tr><tr><td>eval/runtime</td><td>0.745</td></tr><tr><td>eval/samples_per_second</td><td>340.923</td></tr><tr><td>eval/steps_per_second</td><td>10.738</td></tr><tr><td>total_flos</td><td>322376820579840.0</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>+8</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">klue-bert-base</strong> at: <a href='https://wandb.ai/dr-hong/dr-hong/runs/ygo2xa8z' target=\"_blank\">https://wandb.ai/dr-hong/dr-hong/runs/ygo2xa8z</a><br> View project at: <a href='https://wandb.ai/dr-hong/dr-hong' target=\"_blank\">https://wandb.ai/dr-hong/dr-hong</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250910_014143-ygo2xa8z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# wandb ì´ˆê¸°í™”\n",
    "wandb.init(\n",
    "    entity=\"dr-hong\",\n",
    "    project=\"dr-hong\",\n",
    "    name=\"klue-bert-base\",\n",
    "    config={\n",
    "        \"learning_rate\": 3e-5,\n",
    "        \"epochs\": 3,\n",
    "        \"batch_size\": 32,\n",
    "        \"model_name\": \"klue/bert-base\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# compute_metrics í•¨ìˆ˜ ì •ì˜. bert ì— ì í•©í•œ í‰ê°€ì§€í‘œ ì„ íƒ: accuracy, f1, precision, recall\n",
    "def compute_metrics(eval_pred, batch_eval_metrics=False):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = predictions.argmax(axis=1)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# í•™ìŠµ ì¸ì ì •ì˜\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/\" + \"klue/bert-base\".replace(\"/\", \"--\"),\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=1,\n",
    "    eval_delay=32,\n",
    "    learning_rate=3e-5, # differential learning rate ì ìš© ê³ ë ¤í•´ ë³¼ ìˆ˜ ìˆìŒ\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_dir=\"./logs/\" + \"klue/bert-base\".replace(\"/\", \"--\"),\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=16,\n",
    "    save_total_limit=1,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    eval_steps=8,\n",
    "    dataloader_num_workers=4,\n",
    "    disable_tqdm=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"all\",\n",
    "    dataloader_persistent_workers=True,\n",
    "    resume_from_checkpoint=\"./results/\" + \"klue/bert-base\".replace(\"/\", \"--\"),\n",
    "    gradient_checkpointing=True,\n",
    "    auto_find_batch_size=True,\n",
    "    torchdynamo=\"inductor\",\n",
    "    torch_compile=True,\n",
    "    torch_compile_backend=\"inductor\",\n",
    "    torch_compile_mode=\"default\",\n",
    ")\n",
    "\n",
    "# Trainer ì„¤ì •\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_cls_train_datasets,\n",
    "    eval_dataset=tokenized_cls_eval_datasets,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "try:\n",
    "    train_output = trainer.train()\n",
    "\n",
    "    eval_results = trainer.evaluate()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
    "    raise e\n",
    "\n",
    "# ìµœì¢… ëª¨ë¸ ì €ì¥\n",
    "model.save_pretrained(\"./finetuned_model/\" + \"klue/bert-base\".replace(\"/\", \"--\"))\n",
    "\n",
    "# wandb ì¢…ë£Œ\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "execution_state": "idle",
   "id": "2b56a4d2-dd9a-41ae-90f1-60441424a96c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T01:56:29.001973Z",
     "iopub.status.busy": "2025-09-10T01:56:29.001745Z",
     "iopub.status.idle": "2025-09-10T01:56:33.508270Z",
     "shell.execute_reply": "2025-09-10T01:56:33.507569Z",
     "shell.execute_reply.started": "2025-09-10T01:56:29.001958Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template: {\"title\": \"íšŒì‚¬ì†Œê°œì„œ ë°œì†¡\", \"text\": \"ì•ˆë…•í•˜ì„¸ìš” #{ìˆ˜ì‹ ìëª…}ë‹˜, ì €í¬ íšŒì‚¬ë¥¼ ì†Œê°œë“œë¦½ë‹ˆë‹¤.\"}\n",
      "Prediction: Approved\n",
      "Confidence: 0.9935\n",
      "Probabilities: {'Not Approved': 0.0064859273843467236, 'Approved': 0.9935140609741211}\n"
     ]
    }
   ],
   "source": [
    "# ì¶”ë¡  í…ŒìŠ¤íŠ¸\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "\n",
    "# í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ\n",
    "base_model_path = \"./downloaded_model/\" + \"klue/bert-base\".replace(\"/\", \"--\")\n",
    "finetuned_model_path = \"./finetuned_model/\" + \"klue/bert-base\".replace(\"/\", \"--\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(finetuned_model_path)\n",
    "model.eval()\n",
    "\n",
    "def predict_template(template, reject_reason=None):\n",
    "    # í† í°í™”\n",
    "    if reject_reason and not pd.isna(reject_reason):\n",
    "        inputs = tokenizer(\n",
    "            text=template,\n",
    "            text_pair=str(reject_reason),\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    else:\n",
    "        inputs = tokenizer(\n",
    "            text=template,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    \n",
    "    # ì¶”ë¡ \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(predictions, dim=-1).item()\n",
    "        confidence = predictions[0][predicted_class].item()\n",
    "    \n",
    "    return {\n",
    "        \"prediction\": \"Approved\" if predicted_class == 1 else \"Not Approved\",\n",
    "        \"confidence\": confidence,\n",
    "        \"probabilities\": {\n",
    "            \"Not Approved\": predictions[0][0].item(),\n",
    "            \"Approved\": predictions[0][1].item()\n",
    "        }\n",
    "    }\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì˜ˆì‹œ\n",
    "test_template = '{\"title\": \"íšŒì‚¬ì†Œê°œì„œ ë°œì†¡\", \"text\": \"ì•ˆë…•í•˜ì„¸ìš” #{ìˆ˜ì‹ ìëª…}ë‹˜, ì €í¬ íšŒì‚¬ë¥¼ ì†Œê°œë“œë¦½ë‹ˆë‹¤.\"}'\n",
    "result = predict_template(test_template)\n",
    "print(f\"Template: {test_template}\")\n",
    "print(f\"Prediction: {result['prediction']}\")\n",
    "print(f\"Confidence: {result['confidence']:.4f}\")\n",
    "print(f\"Probabilities: {result['probabilities']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dr-hong-pr",
   "language": "python",
   "name": "dr-hong-pr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
